<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="John Doe">
  <meta name="keywords" content="">
  <title>A Neural Probabilistic Language Model - Hexo</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>
<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Fluid</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/A-Neural-Probabilistic-Language-Model/figure1.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-07-19 17:22">
      2020年7月19日 下午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      4k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      47
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
            <article class="markdown-body">
              <h1 id="A-Neural-Probabilistic-Language-Model"><a href="#A-Neural-Probabilistic-Language-Model" class="headerlink" title="A Neural Probabilistic Language Model"></a>A Neural Probabilistic Language Model</h1><p>​    本<a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" target="_blank" rel="noopener">paper</a>由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​    统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，导致目标非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列（这样导致高纬度下的数据系数会导致很多单词序列的概率为0）。<span style="color:red;">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）根据其分布式表示得到的单词序列的概率函数。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​    语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。</p>
<p>​    统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\hat{P}(w^T_1) = \prod^T_{t=1} \hat{P}(w_t|w^{t-1}<em>1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w</em>{i+1}, …, w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。</p>
<p>​    当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个上下文，<code>n-gram</code>模型构建了上下文和下一个单词的条件概率表，例如前$n-1$个单词的组合：$$\hat{p}(w_t|w^{t-1}<em>1) \approx \hat{p}(w_t|w^{t-1}</em>{t-n+1})$$，我们仅考虑在训练预料中真实存在或出现频率足够的连续单词的组合。但是若某个包含<code>n</code>个单词的单词序列未在训练预料中出现过怎么办？对于这种情况，并不能将其概率赋为$0$，因为这样的单词序列是很有可能会在更大的上下文中出现的。一个简单的方法是使用更小的上下文大小来预测其概率，正如论文back-off trigram model(Katz, 1987)以及smoothed(or interpolated) trigram models(Jelinek and Mercer, 1980)所示。因此，在这样的模型中，如何从训练预料中见过的单词序列中获得对新的单词序列的泛化能力呢？一种对此类模型获得泛化能力的理解是将此类interpolated或back-off n-gram模型思考为生成模型。最终，新的单词序列可通过“粘合(gluing)”训练语料中常见的较短、重叠的长度为1,2…到n个单词的序列块生成。在这种back-off或interpolated n-gram模型中，得到下一个单词的规则是隐式的。研究者在使用$n=3$，即trigrams中得到了state-of-art 结果，并且关于结合技巧来得到实质性改进的详细请看Goodman(2001). 显然，距离预测单词越近的前n个单词所包含的信息更多，但上述方法至少有两个方面值得改进，且本文也着重关注：1. 未考虑超过1或者2的更大的上下文；2. 未考虑单词间的相似度。比如，若训练语料中存在句子”The cat is walking in the bedroom”，则类似句子”A dog was running in a room”应该具有更高可能性，因为”dog”和”cat”(“the”和”a”, “room” 和”bedroom” 等等)在语义和语法上很相似。</p>
<p><strong>Contributions</strong></p>
<ol>
<li>提出单词的分布式表示，解决维度灾难问题</li>
<li>解决单词的长距离依赖限制，而ngram模型中一般$n=3$</li>
<li>词的相似关系，本论文中单词以分布式表示，能够表示单词间的相似性</li>
</ol>
<h3 id="1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations"><a href="#1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations" class="headerlink" title="1.1 Fighting the Curse of Dimensionality with Distributed Representations"></a>1.1 Fighting the Curse of Dimensionality with Distributed Representations</h3><p>​    简而言之，本文提出的方法可归纳为如下几点</p>
<ol>
<li>将词典中的每个词都关联到一个分布式的<em>word feature vector</em>（一个$\mathbb{R}^m$空间的实值向量）</li>
<li>单词序列中的联合<em>probability function</em>通过单词的特征向量表征</li>
<li>同时学习<em>word feature vectors</em>和<em>probability function</em>的参数</li>
</ol>
<p>​    单词的特征向量表征单词的不同方面：每个单词都关联到向量空间中的某个点。特征数目（比如m=30,60或者100）远小于词典大小（比如17000）。概率函数表示为在给定之前单词序列时，预测下一个单词的条件概率的乘积（比如实验中在给定先前单词时，采用了多层神经网络预测下一个单词）。概率函数的参数可被迭代调整以达到<strong>maxmize the log-likelihood of training data最大化训练数据的似然函数</strong>或者正则标准，比如增加一个权重衰减惩罚项。最后，每个单词的特征向量都可被学习，当然可采用语义特征的先验知识初始化。</p>
<p>​    该方法为什么有效呢？在先前例子所示，若<em>dog<em>和</em>cat*的语法和语义相似，并且</em>(the, a), (bedroom, room), (is, was), (running, walking)<em>也类似，则很自然的可从*</em>The cat is walking in the bedroom** 生成<strong>A dog was running in a room</strong>，并且类似的<strong>The cat is running in a room</strong>、<strong>A dog is walking in a bedroom</strong>等等等其他各种组合。本文提出的模型是可以泛化的，因为<strong>相似的</strong>单词期望拥有相似的特征向量，并且概率函数是特征向量的<em>smooth平滑</em>函数，即特征上的微小变化也只会引起概率的微小变化。因此，训练数据中的上述句子的出现会增加概率，当然不仅仅是上述句子，还包括其在语句空间中组合数目的<em>邻居</em>句子（通过特征向量表达的句子）</p>
<h3 id="1-2-Relation-to-Previous-Work"><a href="#1-2-Relation-to-Previous-Work" class="headerlink" title="1.2 Relation to Previous Work"></a>1.2 Relation to Previous Work</h3><p>略</p>
<h2 id="2-A-Neural-Model"><a href="#2-A-Neural-Model" class="headerlink" title="2. A Neural Model"></a>2. A Neural Model</h2><p>​    训练集是诸如$w_1 … w_T$的单词序列，其中单词$w_t \in V$，而词典$V$是一个巨大但有限的集合。目标是学习一个好模型<br>$$<br>f(w_t, …, w_{t-n+1}) = \hat{p}(w_t|w^{t-1}<em>1)<br>$$<br>下面，我们给出概率的几何倒数$\frac{1}{\hat{p}(w_t|w^{t-1}_1)}$，也被称为<em>perplexity困惑度</em>，也是平均负似然函数的指数。模型的唯一约束在于对于任何的$w^{t-1}_1$， $\sum^{|V|}</em>{i=1}f(i, w_{t-1}, …, w_{t-n+1}) = 1, f &gt; 0$，通过将这些条件概率相乘，得到单词序列的联合概率模型。</p>
<p>​    我们将函数$f(w_t, …, w_{t-n+1}=\hat{p}(w_t|w^{t-1}_1)$分解为两部分</p>
<ol>
<li><p>一个映射函数$C$， 它将词典$V$中的任意元素$i$映射为一个实值向量$C(i) \in \mathbb{R}^m$。它表示了词典中每个单词的<em>分布式特征向量 distributed feature vectors</em>。实际上，$C$是一个无参数的$|V| * m$的矩阵</p>
</li>
<li><p>一个由$C$表达的单词的概率函数：上下文单词的特征向量序列$(C(w_{t-n+1}), …, C(w_{t-1}))$作为函数$g$的输入，映射到词典$V$中下一个单词$w_t$的条件概率分布。函数$g$的输出是一个向量，其中第$i$-th 元素表示概率$\hat{p}(w_t = i|w^{t-1}<em>1)$ ，如图1所示<br>$$<br>f(i, w</em>{t-1}, …, w_{t-n+1}) = g(i, C(w_{t-1}), …, C(w_{t-n+1}))<br>$$</p>
</li>
</ol>
<p>函数$f$是上述两个映射($C$和$g$)的组合，并且$C$对于上下文中的所有单词都是共享参数的。这两部分都与一些参数有关，映射函数$C$的参数就是特征向量本身，表示为一个$|V|*m$的矩阵，其中第$i$行$C(i)$是单词$i$的特征向量。函数$g$可通过一个前向传播网络或RNN或其他参数化函数实现，参数为$\omega$。整体的参数集合为$\Theta = (C, \omega)$</p>
<p><img src="/img/A-Neural-Probabilistic-Language-Model/figure1.jpg" srcset="/img/loading.gif" alt=""></p>
<p>​    训练过程是寻找最大化训练语料的penalized log-likelihood的参数$\Theta$,<br>$$<br>L = \frac{1}{T}\sum_t log f(w_t, …, w_{t-n+1}; \Theta) + R(\Theta)<br>$$<br>其中$R(\Theta)$是正则化项，比如，在实验中，$R$是一个权重衰减惩罚项，只作用于神经网络的权重和矩阵$C$，不包括偏置项biases</p>
<p>​    在上述模型中，自由参数(free parameters)的数目只与词汇表大小$V$成线性关系。它也只与阶数$n$成线性关系：若引入更多共享参数结构，则比例因子可降为sub-linear次线性，比如使用time-decay神经网络或rnn（或两种网络的组合形式）</p>
<p>​    在如下大多数实验中，除了word features mapping外，神经网络只有一个隐藏层，并且可选性的，存在从word features到输出的直接连接。因此，实际上，神经网络有两个隐藏层：1. the shared word features layer C 即共享单词特征向量层$C$， 没有任何非线性能力（无实质作用）2. the ordinay hyperbolic tangent hidden layer即普通双曲正切隐藏层。更准确的讲，神经网络计算如下函数，使用<em>softmax</em>输出层，保证概率值为正，且和为1<br>$$<br>\hat{p}(w_t|w_{t-1}, …, w_{t-n+1}) = \frac{e^{y_{w_t}}}{\sum_i e^{y_i}}<br>$$<br>其中$y_i$是每个输出单词$i$的未归一化的log-概率，包含参数$b, W, U, d, H$的计算公式如下<br>$$<br>y = b + Wx + Utanh(d + Hx)<br>$$<br>其中，双曲正切tanh是element-by-element方式作用，$W$可选为0，表示无直接连接。$x$为word features layer单词特征向量层的激活向量，是矩阵$C$上的输入单词的特征向量的拼接向量<br>$$<br>x = (C(w_{t-1}), C(w_{t-2}), …, C(w_{t-n+1}))<br>$$<br>设$h$为隐藏层单元数目，$m$为每个单词的特征向量的维度数目。若不存在word features到输出的直接连接，则矩阵$W$为0.模型的自由参数是输出层偏置项$b$（包含$|V|$个元素），隐藏层偏置项$d$（包含$h$个元素），隐藏层到输出层的权重$U$（一个$|V| \times h$的矩阵），word features 到输出的权重$W$（一个$|V| \times (n-1)m$的矩阵），隐藏层权重$H$（一个$h \times (n-1)m$的矩阵），以及word features $C$（一个$|V| \times m$的矩阵）<br>$$<br>\Theta = (b, d, W, U, H, C)<br>$$<br>自由参数的数目是$|V|(1 + nm + h) + h(1 + (n-1)m)$。主导因子是$|V|(nm + h)$，注意在里乱上，若只对权重$W$和$H$而不包含$C$上存在权重衰减，则$W$和$H$会收敛到0，而$C$会继续增长，当然在实际中使用随机梯度上升不会存在这种现象。</p>
<p>​    在神经网络上执行随机梯度上升算法，则执行第t个单词的迭代更新公式如下<br>$$<br>\Theta \leftarrow \Theta + \epsilon \frac{\partial \hat{P}(w_t|w_{t-1}, …, w_{t-n+1})}{\partial \Theta}<br>$$<br>其中$\epsilon$表示学习率，注意大量的参数无需更新：对于大量单词$j$的$C(j)$单词特征没有出现在输入窗口中。</p>
<h2 id="3-Parallel-Implementation"><a href="#3-Parallel-Implementation" class="headerlink" title="3. Parallel Implementation"></a>3. Parallel Implementation</h2><p>略</p>
<h2 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h2><p>​    对比实验使用包含各种英语文本和书籍的共计1181041个单词的布朗语料库，前800000单词用作训练，接下来200000用作验证集（模型选择、权重衰减、early stopping），剩余181041用作测试。不同单词的数目有47578（包括标点符号、大小写以及文本段落标识符），将频率&lt;=3的稀有词合并为单个词，减小词表大小为$|V|=16383$</p>
<p>​    第二个实验使用1995到1996年的Associated Press(AP) News。训练集大约为1400万（13994528）单词，验证集约为100万（963138）单词，测试集也约为100万（963071）单词。原始数据共计148721单词（包括标点符号），减少为$|V|=17964$，处理方法是保留常用词（保留标点符号），大写转为小写，数字映射为特殊符号，稀有词及专有词映射为特殊符号</p>
<p>​    为了训练神经网络，初始学习率设为$\epsilon_o = 10^{-3}$（在使用多个小数据尝试后），并根据如下规则衰减$\epsilon_t = \frac{\epsilon_o}{1 + rt}$，t表示完成的参数更新次数，r表示参数衰减因子 $r = 10^{-8}$</p>
<h3 id="4-1-N-Gram-Models"><a href="#4-1-N-Gram-Models" class="headerlink" title="4.1 N-Gram Models"></a>4.1 N-Gram Models</h3><p>第一个对比模型是采用插值法（interpolated）或平滑法（smoothed）的trigram 模型（Jelinek and Mercer, 1980）。模型的条件概率为<br>$$<br>\hat{p}(w_t|w_{t-1}, w_{t-2}) = \alpha_0(q_t)p_0 + \alpha_1(q_t)p_1(w_t) + \alpha_2(q_t)p_2(w_t|w_{t-1}) + \alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})<br>$$<br>其中条件权重$\alpha_i(q_t) \geq 0, \sum_i \alpha_i(q_t) = 1$，$p_0 = 1/|V|$， $p_1(i)$为unigram（单词$i$在训练集中的相对频率），$p_2(i|j)$为bigram（$\frac{词组(j,i)}{词(j)}$），$p_3(i|j,k)$ 为trigram（$\frac{词组(k,j,i)}{词(j,i)}$）.对每个离散值$q_t$都有混合权重$\alpha$，可通过EM算法经过大约5次迭代估算出。</p>
<h3 id="4-2-Results"><a href="#4-2-Results" class="headerlink" title="4.2 Results"></a>4.2 Results</h3><p>下图为不同模型在困惑度上的表现，可以看到神经网络模型表现最好</p>
<p><img src="/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg" srcset="/img/loading.gif" alt=""></p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>​    实验作用在两个语料库中，一个超过100万数据，另一个超过1500万单词，都证明了本方法比state-of-art方法比如smoothed trigram优秀，困惑度低于10%到20%</p>
<p>​    我们坚信，主要原因在于利用了可学习的分布式表示来解决维度灾难，每个训练语句都存在相当的组合数目的其他邻居语句来训练模型</p>
<p>​    当然，模型还存在较多可改进之处，比如架构、计算性能、先验知识等，未来重点可放在改进加速技术，以及不增加训练时间的基础上增加容量（处理数亿个单词或更多）。利用时间结构扩展输入窗口以包含整个段落（无需增加参数数目或计算时间）的简单想法是使用time-delay或者rnn。</p>
<h2 id="6-参考"><a href="#6-参考" class="headerlink" title="6. 参考"></a>6. 参考</h2><ol>
<li><p><a href="https://www.cnblogs.com/Dream-Fish/p/3950024.html" target="_blank" rel="noopener">A Neural Probabilistic Language Model</a></p>
</li>
<li><p><a href="https://wmathor.com/index.php/archives/1442/" target="_blank" rel="noopener">NNLM 的 PyTorch 实现</a></p>
</li>
</ol>
<h2 id="7-代码实现"><a href="#7-代码实现" class="headerlink" title="7. 代码实现"></a>7. 代码实现</h2><p><a href="https://github.com/DreamExplorerX/NLP-Garden/tree/master/1-1.NNLM" target="_blank" rel="noopener">DreamExplorerX-nnlm</a></p>

            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/Deep-Learning/">Deep Learning</a>
                    
                      <a class="hover-with-bg" href="/tags/NLP/">NLP</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/07/14/C-11%E4%BC%AA%E9%9A%8F%E6%9C%BA%E6%95%B0/">
                        <span class="hidden-mobile">C++11伪随机数</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener">
        <span>Fluid</span></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>







  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "A Neural Probabilistic Language Model&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>




















</body>
</html>
