{"meta":{"version":1,"warehouse":"3.0.2"},"models":{"Asset":[{"_id":"themes/fluid/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/default.png","path":"img/default.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/loading.gif","path":"img/loading.gif","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/avatar.png","path":"img/avatar.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/clipboard-use.js","path":"js/clipboard-use.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/img/police_beian.png","path":"img/police_beian.png","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/debouncer.js","path":"js/debouncer.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/lazyload.js","path":"js/lazyload.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/local-search.js","path":"js/local-search.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/fluid/source/lib/hint/hint.min.css","path":"lib/hint/hint.min.css","modified":1,"renderable":1},{"_id":"source/img/A-Neural-Probabilistic-Language-Model/figure1.jpg","path":"img/A-Neural-Probabilistic-Language-Model/figure1.jpg","modified":1,"renderable":0},{"_id":"source/img/c++11-predefined-random-number-engines.jpg","path":"img/c++11-predefined-random-number-engines.jpg","modified":1,"renderable":0},{"_id":"source/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg","path":"img/A-Neural-Probabilistic-Language-Model/table1-results.jpg","modified":1,"renderable":0},{"_id":"source/img/c++11伪随机数.jpg","path":"img/c++11伪随机数.jpg","modified":1,"renderable":0},{"_id":"source/img/c++11-predefined-random-number-distributions.jpg","path":"img/c++11-predefined-random-number-distributions.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"themes/fluid/.gitignore","hash":"bd095eee271360a38772ee1a42d4f000fb722e5f","modified":1594720349867},{"_id":"themes/fluid/README.md","hash":"23729af73d967cd11a84bed7c839454df3169eb9","modified":1594720349867},{"_id":"themes/fluid/LICENSE","hash":"5b919c12e4f5f5cdebb7c17ded4f10f1ebe64811","modified":1594720349808},{"_id":"themes/fluid/_config.yml","hash":"2c1b18b00ed4187fa7b0fd4036fc05f2072bd860","modified":1594720349868},{"_id":"themes/fluid/_static_prefix.yml","hash":"19b4414055b720ece8d78674085f4e1199ccc128","modified":1594720349864},{"_id":"themes/fluid/.editorconfig","hash":"33218fbd623feb43edf5f99f15965392cecc44a6","modified":1594720349865},{"_id":"themes/fluid/.eslintrc","hash":"0a59eec340b771758cf736290272c549d81ef88c","modified":1594720349865},{"_id":"themes/fluid/local-search.xml","hash":"8c96ba6a064705602ce28d096fd7dd9069630a55","modified":1594720349864},{"_id":"themes/fluid/package.json","hash":"d37c2637ed470ed61dd63142ab1291233267abdb","modified":1594720349867},{"_id":"themes/fluid/README_en.md","hash":"7db56a9214405dd3612b808baaf0847ae4986c6a","modified":1594720349868},{"_id":"themes/fluid/gulpfile.js","hash":"dc82b6be72c786721a2f5e2acc10a2a94995c540","modified":1594720349890},{"_id":"source/_posts/A-Neural-Probabilistic-Language-Model.md","hash":"97ba6cbad4a4c6cc33783d12bd4a409224ea14cf","modified":1598327312282},{"_id":"source/_posts/C-11伪随机数.md","hash":"db93dfaf75f13a1cb54e130d4509d6253edcf7fc","modified":1594780824033},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1594720198739},{"_id":"source/_data/_config.yml","hash":"2c1b18b00ed4187fa7b0fd4036fc05f2072bd860","modified":1594729862712},{"_id":"source/about/index.md","hash":"666ba4260baf40b238eaee8b084a460b2f984e85","modified":1594729337512},{"_id":"themes/fluid/languages/en.yml","hash":"a85dcc5cc21f9cab50df31e5001b8818ee62d1e2","modified":1594720349866},{"_id":"themes/fluid/languages/ja.yml","hash":"91020031a847c0361a6fd7ab990c7be4bf17529b","modified":1594720349866},{"_id":"themes/fluid/languages/zh-CN.yml","hash":"21307b4137c3d9b04bb58243747e75af0abc5a71","modified":1594720349866},{"_id":"themes/fluid/layout/404.ejs","hash":"689d9f4efd2a7f5edfd9b24561a7ade69d46617c","modified":1594720349809},{"_id":"themes/fluid/layout/about.ejs","hash":"a23e73c92b6d3ff0df55dbd1aa774065a3291d8d","modified":1594720349812},{"_id":"themes/fluid/layout/archive.ejs","hash":"783ce7efe83bd72e3c767fd5a190ad38113ff34f","modified":1594720349810},{"_id":"themes/fluid/layout/categories.ejs","hash":"3b8589675338c7b3e1849104b7bb15ac477fce4e","modified":1594720349812},{"_id":"themes/fluid/layout/category.ejs","hash":"a3f1195424a24ea2b6d996e504b503e9c156f517","modified":1594720349809},{"_id":"themes/fluid/layout/index.ejs","hash":"31c62687deb7d224c9b7f4e6d0d5f43406dd3fce","modified":1594720349810},{"_id":"themes/fluid/layout/layout.ejs","hash":"941b922aae1e0f8c0dfd57cc14d50c0c85a8ec60","modified":1594720349811},{"_id":"themes/fluid/layout/links.ejs","hash":"0bb8360ad6a2433fdeddf01fbd39b4f2db67b08f","modified":1594720349837},{"_id":"themes/fluid/layout/page.ejs","hash":"8cab50ead4cdb992d35710147a9a5308fb5df290","modified":1594720349838},{"_id":"themes/fluid/layout/post.ejs","hash":"7ecfa13fe363799c7620d3386a79a889d6a7545d","modified":1594720349810},{"_id":"themes/fluid/layout/tag.ejs","hash":"86d1c27b1536eb3f12c18f7264abb1d6fa6d96f6","modified":1594720349811},{"_id":"themes/fluid/layout/tags.ejs","hash":"1d06af34b6cf1d8a20d2eb565e309326ceba309f","modified":1594720349812},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report.md","hash":"7fcdcd44f3a851722e8ffc511b3ed336c30d7561","modified":1594720349888},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/bug_report_zh.md","hash":"272067b338530bee68c0c08b839da552cfe486b4","modified":1594720349885},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request.md","hash":"d3a3204d9bb2b43a69c9cb0be59bada8cb91e412","modified":1594720349886},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/feature_request_zh.md","hash":"a413dc14e4737dbcaa8fb797d37f85121ede6551","modified":1594720349889},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question_zh.md","hash":"e24b470f7aa8044499a4f5e39634e5dc43899011","modified":1594720349887},{"_id":"themes/fluid/.github/ISSUE_TEMPLATE/question.md","hash":"93cf5e1f1e0362adee0f63f4edcc7738cd4fc842","modified":1594720349889},{"_id":"themes/fluid/.github/workflows/lint.yaml","hash":"0c02cc7e5a1318c481a6f523747407b29da58645","modified":1594720349883},{"_id":"themes/fluid/.github/workflows/limit.yaml","hash":"f8bd2edeb4424ee7a055b31583445d5d5dff91a4","modified":1594720349882},{"_id":"themes/fluid/layout/_partial/beian.ejs","hash":"4f3acc8dc822c85d0d778552a15875f71c0cd80a","modified":1594720349834},{"_id":"themes/fluid/layout/_partial/css.ejs","hash":"cc6f4130c14ba7ebdb68bb5664f05dc16d7cecb1","modified":1594720349837},{"_id":"themes/fluid/layout/_partial/footer.ejs","hash":"03defcd4409dce27447aa8dfe7ce41a2d47e373b","modified":1594720349836},{"_id":"themes/fluid/layout/_partial/head.ejs","hash":"5dbb5ae9a8d10b43beb0495788c83629c81e9922","modified":1594720349835},{"_id":"themes/fluid/layout/_partial/nav.ejs","hash":"502f9cdf526e850833d87415cda352a2e1002a51","modified":1594720349817},{"_id":"themes/fluid/layout/_partial/paginator.ejs","hash":"783eee847562ce14db8f723b4ae742fb69aaf620","modified":1594720349817},{"_id":"themes/fluid/layout/_partial/post-meta.ejs","hash":"411fe0b8b169cd4e94d00450ae76a3b7ce994c44","modified":1594720349831},{"_id":"themes/fluid/layout/_partial/scripts.ejs","hash":"0d818a43079d9a2da12d9c974426d675cc0b66ea","modified":1594720349816},{"_id":"themes/fluid/layout/_partial/search.ejs","hash":"cdd7919fa01f6ef7ccc09938d662ff3d77f5d999","modified":1594720349830},{"_id":"themes/fluid/layout/_partial/statistics.ejs","hash":"920bc618d357d48d2b96f8758f6ae8f9488fc4d8","modified":1594720349832},{"_id":"themes/fluid/layout/_partial/toc.ejs","hash":"3cf1d9b9032919d6b936e1f2410911df3b325670","modified":1594720349833},{"_id":"themes/fluid/scripts/events/index.js","hash":"36b137b75c76ff08523016dc8eac702f397d771b","modified":1594720349875},{"_id":"themes/fluid/scripts/filters/post-filter.js","hash":"6665f19fa30cbbe0853d8140a0832f8638538f89","modified":1594720349869},{"_id":"themes/fluid/scripts/generators/local-search.js","hash":"bb90a128a999b276c10a822c44851fa27d6d64ae","modified":1594720349874},{"_id":"themes/fluid/scripts/generators/pages.js","hash":"f64d2121d99225e9f6d12ce646af618232f5f366","modified":1594720349874},{"_id":"themes/fluid/scripts/helpers/page.js","hash":"d45dce23532c17367c7eb86b94afa8c9e40d16c6","modified":1594720349880},{"_id":"themes/fluid/scripts/helpers/url.js","hash":"99ab4551dc9c035abcc3bf4da5def2f63449d7ec","modified":1594720349881},{"_id":"themes/fluid/scripts/helpers/utils.js","hash":"9045f47c7a71aab39f16cffb3e3847b752c2e0f1","modified":1594720349880},{"_id":"themes/fluid/scripts/helpers/wordcount.js","hash":"e58d422eddb44c1be893f65f79f4c7feecfe6d5f","modified":1594720349879},{"_id":"themes/fluid/scripts/tags/button.js","hash":"48e1b46b4c34b79a128dda7a0592b43be47cf955","modified":1594720349872},{"_id":"themes/fluid/scripts/tags/checkbox.js","hash":"c131fb22805cec89d647e643299508487fc72576","modified":1594720349871},{"_id":"themes/fluid/scripts/tags/group-image.js","hash":"4aeebb797026f1df25646a5d69f7fde79b1bcd26","modified":1594720349871},{"_id":"themes/fluid/scripts/tags/label.js","hash":"d50f5aeb1a95adbc88cea9cca4a07165d6725408","modified":1594720349873},{"_id":"themes/fluid/scripts/tags/note.js","hash":"0886cfe3f8589671a1d289495e359c20a9908080","modified":1594720349872},{"_id":"themes/fluid/scripts/utils/join-path.js","hash":"629e7deb3955f750c1cfa6fc773f412e020fcef4","modified":1594720349870},{"_id":"themes/fluid/scripts/utils/object.js","hash":"61e9555f99edcb23d55361c7154e23af33153ecb","modified":1594720349869},{"_id":"themes/fluid/source/css/main.styl","hash":"d5a8a59c8d1fd17d699a951e59c4ce9ae44c419d","modified":1594720349855},{"_id":"themes/fluid/source/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1594720349863},{"_id":"themes/fluid/source/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1594720349862},{"_id":"themes/fluid/source/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1594720349860},{"_id":"themes/fluid/source/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1594720349861},{"_id":"themes/fluid/source/js/clipboard-use.js","hash":"f0ffe3df1deeb3cc36adfee2ab839368fb28d1c9","modified":1594720349860},{"_id":"themes/fluid/source/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1594720349862},{"_id":"themes/fluid/source/js/debouncer.js","hash":"045f324777bdfb99d4c17b1806169f029f897a65","modified":1594720349859},{"_id":"themes/fluid/source/js/lazyload.js","hash":"792deca48c12512d3e612f0de092ffcff3ca3e34","modified":1594720349858},{"_id":"themes/fluid/source/js/local-search.js","hash":"5e36b80ad9594246f78da743a742527382b7cd50","modified":1594720349858},{"_id":"themes/fluid/source/js/main.js","hash":"863a68ca6fac43dc2a42f6ffcad2ccbdd7d249cc","modified":1594720349859},{"_id":"themes/fluid/source/js/utils.js","hash":"24b1f871f74f9bed7ac59bb0ad28a60f1491e4c2","modified":1594720349860},{"_id":"themes/fluid/layout/_partial/comments/disqus.ejs","hash":"3f2b188bc108d8354ffd9d74511125d50ec02979","modified":1594720349814},{"_id":"themes/fluid/layout/_partial/comments/changyan.ejs","hash":"fe37916cb431c8842427797aa1e4edf55b30af0a","modified":1594720349813},{"_id":"themes/fluid/layout/_partial/comments/gitalk.ejs","hash":"0a34a8c809209089466116c46d7a2daac5a4f326","modified":1594720349814},{"_id":"themes/fluid/layout/_partial/comments/livere.ejs","hash":"872f8915a791c6015017d026225de6bc76441a8a","modified":1594720349816},{"_id":"themes/fluid/layout/_partial/comments/utterances.ejs","hash":"4171a45f75525251ffd4e15756b34e5b9c4cb167","modified":1594720349815},{"_id":"themes/fluid/layout/_partial/comments/valine.ejs","hash":"3a13813e62f0b5f57ae4b9d6b7c9b32d7bca70d6","modified":1594720349815},{"_id":"themes/fluid/layout/_partial/plugins/analytics.ejs","hash":"db103077a6957c9d4786e98fd8144d6a5b03c495","modified":1594720349824},{"_id":"themes/fluid/layout/_partial/plugins/anchor.ejs","hash":"3738c2ef427b4b400225b92e638a17b7ab2125ed","modified":1594720349829},{"_id":"themes/fluid/layout/_partial/plugins/aplayer.ejs","hash":"e843cdbe64af3dee9385eb1d763539d3ac10ed72","modified":1594720349818},{"_id":"themes/fluid/layout/_partial/plugins/daovoice.ejs","hash":"cfc684ba48608abd25afd155ee373d9936bbe84e","modified":1594720349820},{"_id":"themes/fluid/layout/_partial/plugins/fancybox.ejs","hash":"c447e35c93c61a70c1c2dfc34948615832989660","modified":1594720349819},{"_id":"themes/fluid/layout/_partial/plugins/leancloud.ejs","hash":"7807e58722dcf992fba11bc879b0fac0e904dc76","modified":1594720349821},{"_id":"themes/fluid/layout/_partial/plugins/local-search.ejs","hash":"03b024c7bab51b64e5187c9dbfac039d020f1e97","modified":1594720349822},{"_id":"themes/fluid/layout/_partial/plugins/mermaid.ejs","hash":"10ed1f9a611449d37736e17c4e251127b38b3772","modified":1594720349823},{"_id":"themes/fluid/layout/_partial/plugins/math.ejs","hash":"76c4e0608ae362a265ac5e9c0fc49f75c1bc568e","modified":1594720349823},{"_id":"themes/fluid/layout/_partial/plugins/mouse-click.ejs","hash":"eb19991199c201ceb103a6ef025e4dfd1e7dbfb7","modified":1594720349826},{"_id":"themes/fluid/layout/_partial/plugins/tocjs.ejs","hash":"4d961ce5cc706445f12b9636801e80a521579f72","modified":1594720349827},{"_id":"themes/fluid/layout/_partial/plugins/typed.ejs","hash":"38334350425008d1f64323de221457ee0948af0e","modified":1594720349828},{"_id":"themes/fluid/scripts/events/lib/footnote.js","hash":"3b2abc5f5e3b681874637e98e047dc4969eb1983","modified":1594720349876},{"_id":"themes/fluid/scripts/events/lib/hello.js","hash":"da4c281a8b2d2ed813da1236950c9dff87334adc","modified":1594720349876},{"_id":"themes/fluid/scripts/events/lib/lazyload.js","hash":"ee7dfef805af96195c11223cca53983deace8bda","modified":1594720349875},{"_id":"themes/fluid/scripts/events/lib/highlight.js","hash":"a382f567d95e53ef614246098fd844dd3b86ac44","modified":1594720349878},{"_id":"themes/fluid/scripts/events/lib/merge-configs.js","hash":"374583fd419ac6477dfca69e09756db543d7ec80","modified":1594720349877},{"_id":"themes/fluid/scripts/events/lib/version.js","hash":"5fed24e25662a1b08fd857fde2d770fdc6250475","modified":1594720349876},{"_id":"themes/fluid/source/css/_functions/base.styl","hash":"2e46f3f4e2c9fe34c1ff1c598738fc7349ae8188","modified":1594720349840},{"_id":"themes/fluid/source/css/_mixins/base.styl","hash":"542e306ee9494e8a78e44d6d7d409605d94caeb3","modified":1594720349857},{"_id":"themes/fluid/source/css/_pages/pages.styl","hash":"b8e887bc7fb3b765a1f8ec9448eff8603a41984f","modified":1594720349847},{"_id":"themes/fluid/source/css/_variables/base.styl","hash":"3cca1d1ed5fe23d9025ec5fe28d2a215b93efade","modified":1594720349841},{"_id":"themes/fluid/source/lib/hint/hint.min.css","hash":"b38df228460ebfb4c0b6085336ee2878fe85aafe","modified":1594720349863},{"_id":"themes/fluid/source/css/_pages/_category/category.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1594720349848},{"_id":"themes/fluid/source/css/_pages/_tag/tag.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1594720349846},{"_id":"source/img/A-Neural-Probabilistic-Language-Model/figure1.jpg","hash":"9dad57d7fdf89b1e573ec943c0932ae3c96f97e8","modified":1598254826303},{"_id":"themes/fluid/source/css/_pages/_about/about.styl","hash":"7e07e44fa0e77ddbdd3aa0c0abdb3be6822df2e0","modified":1594720349843},{"_id":"themes/fluid/source/css/_pages/_archive/archive.styl","hash":"6e6f22b664199772370b59ce1678b0c148b5849f","modified":1594720349850},{"_id":"themes/fluid/source/css/_pages/_base/base.styl","hash":"2637dce25ee7c00b94e64d15834d178f3c78a912","modified":1594720349852},{"_id":"themes/fluid/source/css/_pages/_base/keyframes.styl","hash":"c34979b9b8e5c9cb54dcdd73a1fe877bcb943548","modified":1594720349853},{"_id":"themes/fluid/source/css/_pages/_base/rewrite.styl","hash":"7288884ed0f3bf5e4d8ae790269357ca073eab64","modified":1594720349852},{"_id":"themes/fluid/source/css/_pages/_category/categories.styl","hash":"4f27a4bdf703aec6f900727df2596bac902c441a","modified":1594720349849},{"_id":"themes/fluid/source/css/_pages/_index/index.styl","hash":"9ce4d5d708047520345a85bbfc101b3e67dbf7b8","modified":1594720349851},{"_id":"themes/fluid/source/css/_pages/_links/links.styl","hash":"a13243e6281432e312ea73683cfd1a3dd50f6ebf","modified":1594720349854},{"_id":"themes/fluid/source/css/_pages/_post/post.styl","hash":"63152526c501e674085235982603ba60b659cb84","modified":1594720349844},{"_id":"themes/fluid/source/css/_pages/_post/tag_plugin.styl","hash":"37043762d345f33dc1817c18e4ac6c47551f6d42","modified":1594720349845},{"_id":"themes/fluid/source/css/_pages/_tag/tags.styl","hash":"934a6ae74b29a6903d2309498653910c19a77c8b","modified":1594720349847},{"_id":"source/img/c++11-predefined-random-number-engines.jpg","hash":"e8c6d3a61fdaa6fbfeec3eb5e97a3122f3587323","modified":1594775923292},{"_id":"source/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg","hash":"78f5162a2114ba6107fe6d2313a810c5b1474489","modified":1598322751843},{"_id":"source/img/c++11伪随机数.jpg","hash":"71fb0d348db51c4519ddad0872e26fde700e3966","modified":1594730032499},{"_id":"source/img/c++11-predefined-random-number-distributions.jpg","hash":"0a53fd1ca2c936ab2651cdfb13b45ddb8bb47b73","modified":1594779522391},{"_id":"public/local-search.xml","hash":"44a2f754edc789f62ca01e07389c1a881f37cd21","modified":1598327330194},{"_id":"public/about/index.html","hash":"9c3bae3dddf1e98ebe3246471b148a350821657c","modified":1598327330194},{"_id":"public/2020/07/14/hello-world/index.html","hash":"4a268777c484bd1e2a4b624cd46587a8b72ec9c4","modified":1598327330194},{"_id":"public/archives/index.html","hash":"982642b85377179800ba5a0faefb78f548bc7847","modified":1598327330194},{"_id":"public/archives/2020/index.html","hash":"982642b85377179800ba5a0faefb78f548bc7847","modified":1598327330194},{"_id":"public/archives/2020/07/index.html","hash":"982642b85377179800ba5a0faefb78f548bc7847","modified":1598327330194},{"_id":"public/categories/C-11/index.html","hash":"c936022ed23969ef3cb878a3ffff080e4c255d44","modified":1598327330194},{"_id":"public/tags/C-11/index.html","hash":"858978b5f2250c2e6fca989441b4f2b986e2d7da","modified":1598327330194},{"_id":"public/tags/random/index.html","hash":"bc900d7209f32362d0d345e697f2dec7f1edbef9","modified":1598327330194},{"_id":"public/tags/Deep-Learning/index.html","hash":"828e764e0867c025f7919a1ddaac2900e66b7afc","modified":1598327330194},{"_id":"public/tags/NLP/index.html","hash":"4af9118cdcfa5756bf304b2ab340c5aabfe2a949","modified":1598327330194},{"_id":"public/index.html","hash":"584d1c826f236ad29e260725bdffd02d79810853","modified":1598327330194},{"_id":"public/404.html","hash":"121bd4fa6ca201ab087f99ff1d550fffd970ac6c","modified":1598327330194},{"_id":"public/tags/index.html","hash":"92517d1049a446e8d6e9c526315f0486dcd0ba78","modified":1598327330194},{"_id":"public/categories/index.html","hash":"b43b54a30755f06a2da0dd2207106d631da66d7a","modified":1598327330194},{"_id":"public/links/index.html","hash":"2209822e04162cd9349ed8befebf2e091e66c411","modified":1598327330194},{"_id":"public/2020/07/19/A-Neural-Probabilistic-Language-Model/index.html","hash":"895ffb00d8969aaae32d31687b719d199a7e3fcb","modified":1598327330194},{"_id":"public/2020/07/14/C-11伪随机数/index.html","hash":"d4536b504ac4c38b6c49ef448da0e45f293373ed","modified":1598327330194},{"_id":"public/img/favicon.png","hash":"64b215db2cb3af98fe639e94537cb5209f959c78","modified":1598327330194},{"_id":"public/img/avatar.png","hash":"fe739a158cc128f70f780eb5fa96f388b81d478f","modified":1598327330194},{"_id":"public/img/police_beian.png","hash":"90efded6baa2dde599a9d6b1387973e8e64923ea","modified":1598327330194},{"_id":"public/img/default.png","hash":"7bb2b8ee07db305bcadee2985b81b942027ae940","modified":1598327330194},{"_id":"public/img/loading.gif","hash":"2d2fc0f947940f98c21afafef39ecf226a2e8d55","modified":1598327330194},{"_id":"public/js/clipboard-use.js","hash":"f0ffe3df1deeb3cc36adfee2ab839368fb28d1c9","modified":1598327330194},{"_id":"public/js/debouncer.js","hash":"045f324777bdfb99d4c17b1806169f029f897a65","modified":1598327330194},{"_id":"public/js/main.js","hash":"863a68ca6fac43dc2a42f6ffcad2ccbdd7d249cc","modified":1598327330194},{"_id":"public/js/local-search.js","hash":"5e36b80ad9594246f78da743a742527382b7cd50","modified":1598327330194},{"_id":"public/js/utils.js","hash":"24b1f871f74f9bed7ac59bb0ad28a60f1491e4c2","modified":1598327330194},{"_id":"public/lib/hint/hint.min.css","hash":"b38df228460ebfb4c0b6085336ee2878fe85aafe","modified":1598327330194},{"_id":"public/js/lazyload.js","hash":"792deca48c12512d3e612f0de092ffcff3ca3e34","modified":1598327330194},{"_id":"public/css/main.css","hash":"6fb3fbe84a37903b88f2348e2954e66b9b70fe0a","modified":1598327330194},{"_id":"public/img/A-Neural-Probabilistic-Language-Model/figure1.jpg","hash":"9dad57d7fdf89b1e573ec943c0932ae3c96f97e8","modified":1598327330194},{"_id":"public/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg","hash":"78f5162a2114ba6107fe6d2313a810c5b1474489","modified":1598327330194},{"_id":"public/img/c++11-predefined-random-number-engines.jpg","hash":"e8c6d3a61fdaa6fbfeec3eb5e97a3122f3587323","modified":1598327330194},{"_id":"public/img/c++11伪随机数.jpg","hash":"71fb0d348db51c4519ddad0872e26fde700e3966","modified":1598327330194},{"_id":"public/img/c++11-predefined-random-number-distributions.jpg","hash":"0a53fd1ca2c936ab2651cdfb13b45ddb8bb47b73","modified":1598327330194}],"Category":[{"name":"C++11","_id":"cke9etpe10002zdr95kglf1rx"}],"Data":[{"_id":"_config","data":{"favicon":"/img/favicon.png","apple_touch_icon":"/img/favicon.png","title_join_string":" - ","force_https":false,"highlight":{"enable":true,"style":"Github Gist","bg_color":false,"copy_btn":true},"fun_features":{"typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"mouse_click":{"enable":false,"style":"values"}},"color":{"body_bg_color":"#eee","navbar_bg_color":"#2f4154","navbar_text_color":"white","text_color":"#3c4858","sec_text_color":"#718096","post_text_color":"#2c3e50","post_heading_color":"#1a202c","link_color":"#3c4858","link_hover_color":"#1abc9c","link_hover_bg_color":"#f8f9fa","board_color":"#fff"},"font":{"font_size":"16px","font_family":null,"code_font_size":"85%"},"custom_js":null,"custom_css":null,"custom_html":"","web_analytics":{"enable":false,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"tajs":null,"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"lazyload":{"enable":true,"onlypost":false},"daovoice":{"enable":false,"appid":""},"aplayer":{"enable":false,"autoplay":false,"loop":"all","order":"random","theme":"#b7daff","songs":[{"name":"name","artist":"artist","url":"/songs/test.mp3","cover":"/img/cover.jpg"},{"name":"name","artist":"artist","url":"https://...url.mp3","cover":"https://...cover.jpg"}]},"version":{"check":false},"navbar":{"blog_title":"Fluid","ground_glass":{"enable":false,"px":3,"alpha":0.7},"menu":[{"key":"home","link":"/","icon":"iconfont icon-home-fill"},{"key":"archive","link":"/archives/","icon":"iconfont icon-archive-fill"},{"key":"category","link":"/categories/","icon":"iconfont icon-category-fill"},{"key":"tag","link":"/tags/","icon":"iconfont icon-tags-fill"},{"key":"about","link":"/about/","icon":"iconfont icon-user-fill"}]},"search":{"enable":true,"path":"/local-search.xml","generate_path":"/local-search.xml","field":"post","content":true},"scroll_down_arrow":{"enable":true,"banner_height_limit":90,"scroll_after_turning_page":true},"banner_parallax":true,"footer":{"statistics":{"enable":false,"source":"busuanzi","pv_format":"总访问量 {} 次","uv_format":"总访客数 {} 人"},"beian":{"enable":false,"icp_text":"京ICP证123456号","police_text":"京公网安备12345678号","police_code":12345678,"police_icon":"/img/police_beian.png"}},"scroll_top_arrow":{"enable":true},"index":{"banner_img":"/img/default.png","banner_img_height":100,"banner_mask_alpha":0.3,"post_default_img":"","slogan":{"enable":true,"text":"An elegant Material-Design theme for Hexo"},"auto_excerpt":{"enable":true},"post_url_target":"_self","post_meta":{"date":true,"category":true,"tag":true}},"page":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3},"post":{"banner_img":"/img/default.png","banner_img_height":70,"banner_mask_alpha":0.3,"meta":{"date":{"enable":true,"format":"LL a"},"wordcount":{"enable":true,"format":"{} 字"},"min2read":{"enable":true,"format":"{} 分钟","words":100},"views":{"enable":false,"source":"busuanzi","format":"{} 次"}},"updated":{"enable":false,"content":"本文最后更新于：","relative":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"copyright":{"enable":true,"content":"本博客所有文章除特别声明外，均采用 <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.zh\" rel=\"nofollow noopener\">CC BY-SA 4.0 协议</a> ，转载请注明出处！"},"prev_next":{"enable":true},"custom":{"enable":false,"content":"<img src=\"https://octodex.github.com/images/jetpacktocat.png\" class=\"rounded mx-auto d-block mt-5\" style=\"width:150px; height:150px;\">"},"image_zoom":{"enable":true},"footnote":{"enable":true,"header":""},"math":{"enable":false,"specific":false,"engine":"mathjax"},"mermaid":{"enable":false,"specific":false,"options":{"theme":"default"}},"comments":{"enable":false,"type":"disqus"}},"utterances":{"repo":null,"issue_term":null,"label":"utterances","theme":"github-light","crossorigin":"anonymous"},"disqus":{"shortname":null,"disqusjs":false,"apikey":null},"gitalk":{"clientID":null,"clientSecret":null,"repo":null,"owner":null,"admin":null,"id":"location.pathname","language":"zh-CN","labels":"['Gitalk']","perPage":15,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true},"valine":{"appid":null,"appkey":null,"placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":null},"changyan":{"appid":"","appkey":""},"livere":{"uid":""},"archive":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null},"category":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"post_limit":10,"order_by":"-length"},"tag":{"banner_img":"/img/default.png","banner_img_height":80,"banner_mask_alpha":0.3,"subtitle":null,"tagcloud":{"min_font":15,"max_font":30,"unit":"px","start_color":"#BBBBEE","end_color":"#337ab7"}},"about":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"avatar":"/img/avatar.png","name":"myname","introduce":"一句简短的介绍","icons":[{"class":"iconfont icon-github-fill","link":"https://github.com","tip":"GitHub"},{"class":"iconfont icon-douban-fill","link":"https://douban.com","tip":"豆瓣"},{"class":"iconfont icon-wechat-fill","qrcode":"/img/favicon.png"}]},"page404":{"banner_img":"/img/default.png","banner_img_height":85,"banner_mask_alpha":0.3,"subtitle":"Page not found"},"links":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"items":[{"title":"Fluid Docs","intro":"主题使用指南","link":"https://hexo.fluid-dev.com/docs/","image":"/img/favicon.png"},{"title":"Fluid Repo","intro":"主题 GitHub 仓库","link":"https://github.com/fluid-dev/hexo-theme-fluid","image":"/img/favicon.png"},{"title":"Fluid Example","intro":"主题操作示例","link":"https://hexo.fluid-dev.com/docs/example/","image":"/img/favicon.png"}]}}}],"Page":[{"title":"about","date":"2020-07-14T12:21:52.000Z","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2020-07-14 20:21:52\nlayout: about\n---\n","updated":"2020-07-14T12:22:17.512Z","path":"about/index.html","comments":1,"_id":"cke9etpf0000dzdr9g0lcfdnw","content":"","site":{"data":{"_config":{"favicon":"/img/favicon.png","apple_touch_icon":"/img/favicon.png","title_join_string":" - ","force_https":false,"highlight":{"enable":true,"style":"Github Gist","bg_color":false,"copy_btn":true},"fun_features":{"typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"mouse_click":{"enable":false,"style":"values"}},"color":{"body_bg_color":"#eee","navbar_bg_color":"#2f4154","navbar_text_color":"white","text_color":"#3c4858","sec_text_color":"#718096","post_text_color":"#2c3e50","post_heading_color":"#1a202c","link_color":"#3c4858","link_hover_color":"#1abc9c","link_hover_bg_color":"#f8f9fa","board_color":"#fff"},"font":{"font_size":"16px","font_family":null,"code_font_size":"85%"},"custom_js":null,"custom_css":null,"custom_html":"","web_analytics":{"enable":false,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"tajs":null,"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"lazyload":{"enable":true,"onlypost":false},"daovoice":{"enable":false,"appid":""},"aplayer":{"enable":false,"autoplay":false,"loop":"all","order":"random","theme":"#b7daff","songs":[{"name":"name","artist":"artist","url":"/songs/test.mp3","cover":"/img/cover.jpg"},{"name":"name","artist":"artist","url":"https://...url.mp3","cover":"https://...cover.jpg"}]},"version":{"check":false},"navbar":{"blog_title":"Fluid","ground_glass":{"enable":false,"px":3,"alpha":0.7},"menu":[{"key":"home","link":"/","icon":"iconfont icon-home-fill"},{"key":"archive","link":"/archives/","icon":"iconfont icon-archive-fill"},{"key":"category","link":"/categories/","icon":"iconfont icon-category-fill"},{"key":"tag","link":"/tags/","icon":"iconfont icon-tags-fill"},{"key":"about","link":"/about/","icon":"iconfont icon-user-fill"}]},"search":{"enable":true,"path":"/local-search.xml","generate_path":"/local-search.xml","field":"post","content":true},"scroll_down_arrow":{"enable":true,"banner_height_limit":90,"scroll_after_turning_page":true},"banner_parallax":true,"footer":{"statistics":{"enable":false,"source":"busuanzi","pv_format":"总访问量 {} 次","uv_format":"总访客数 {} 人"},"beian":{"enable":false,"icp_text":"京ICP证123456号","police_text":"京公网安备12345678号","police_code":12345678,"police_icon":"/img/police_beian.png"}},"scroll_top_arrow":{"enable":true},"index":{"banner_img":"/img/default.png","banner_img_height":100,"banner_mask_alpha":0.3,"post_default_img":"","slogan":{"enable":true,"text":"An elegant Material-Design theme for Hexo"},"auto_excerpt":{"enable":true},"post_url_target":"_self","post_meta":{"date":true,"category":true,"tag":true}},"page":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3},"post":{"banner_img":"/img/default.png","banner_img_height":70,"banner_mask_alpha":0.3,"meta":{"date":{"enable":true,"format":"LL a"},"wordcount":{"enable":true,"format":"{} 字"},"min2read":{"enable":true,"format":"{} 分钟","words":100},"views":{"enable":false,"source":"busuanzi","format":"{} 次"}},"updated":{"enable":false,"content":"本文最后更新于：","relative":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"copyright":{"enable":true,"content":"本博客所有文章除特别声明外，均采用 <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.zh\" rel=\"nofollow noopener\">CC BY-SA 4.0 协议</a> ，转载请注明出处！"},"prev_next":{"enable":true},"custom":{"enable":false,"content":"<img src=\"https://octodex.github.com/images/jetpacktocat.png\" class=\"rounded mx-auto d-block mt-5\" style=\"width:150px; height:150px;\">"},"image_zoom":{"enable":true},"footnote":{"enable":true,"header":""},"math":{"enable":false,"specific":false,"engine":"mathjax"},"mermaid":{"enable":false,"specific":false,"options":{"theme":"default"}},"comments":{"enable":false,"type":"disqus"}},"utterances":{"repo":null,"issue_term":null,"label":"utterances","theme":"github-light","crossorigin":"anonymous"},"disqus":{"shortname":null,"disqusjs":false,"apikey":null},"gitalk":{"clientID":null,"clientSecret":null,"repo":null,"owner":null,"admin":null,"id":"location.pathname","language":"zh-CN","labels":"['Gitalk']","perPage":15,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true},"valine":{"appid":null,"appkey":null,"placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":null},"changyan":{"appid":"","appkey":""},"livere":{"uid":""},"archive":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null},"category":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"post_limit":10,"order_by":"-length"},"tag":{"banner_img":"/img/default.png","banner_img_height":80,"banner_mask_alpha":0.3,"subtitle":null,"tagcloud":{"min_font":15,"max_font":30,"unit":"px","start_color":"#BBBBEE","end_color":"#337ab7"}},"about":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"avatar":"/img/avatar.png","name":"myname","introduce":"一句简短的介绍","icons":[{"class":"iconfont icon-github-fill","link":"https://github.com","tip":"GitHub"},{"class":"iconfont icon-douban-fill","link":"https://douban.com","tip":"豆瓣"},{"class":"iconfont icon-wechat-fill","qrcode":"/img/favicon.png"}]},"page404":{"banner_img":"/img/default.png","banner_img_height":85,"banner_mask_alpha":0.3,"subtitle":"Page not found"},"links":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"items":[{"title":"Fluid Docs","intro":"主题使用指南","link":"https://hexo.fluid-dev.com/docs/","image":"/img/favicon.png"},{"title":"Fluid Repo","intro":"主题 GitHub 仓库","link":"https://github.com/fluid-dev/hexo-theme-fluid","image":"/img/favicon.png"},{"title":"Fluid Example","intro":"主题操作示例","link":"https://hexo.fluid-dev.com/docs/example/","image":"/img/favicon.png"}]}}}},"excerpt":"","more":""}],"Post":[{"title":"C++11伪随机数","date":"2020-07-14T12:11:25.000Z","index_img":"img/c++11伪随机数.jpg","banner_img":"img/c++11伪随机数.jpg","_content":"# C++11 伪随机数\n\n## C风格伪随机数生成\n旧版本C/C++使用`rand()`函数生成区间在`[0, RAND_MAX]`的伪随机数，其存在多种问题：无法满足多种应用程序对不同区间范围、不同数值类型或不同分布的需求。而编程人员在编写程序为了解决这些问题通常会转换 rand 生成的随机数的范围、类型或者是分布时，常常会引入非随机性。\n\n## C++11新标准\nC++11新标准中提供了新的随机数生成库`<random>`来解决上述问题，`<random>`库主要通过`generator`和`distribution`的协作来生成随机数\n\n* random_device类：可能产生真正的随机数，但依具体实现\n\n* random number engines(随机数生成引擎类): 负责生成原始随机数\n* random number distributions(随机数生成分布类): 使得生成的随机数满足统计学中的概率分布\nSTL预先指定了一系列的生成引擎，并且提供一个default_random_engine。default_random_engine会使用某个预定义的引擎，且不同编译器、不同平台的实现可能不同。\n\n### 非确定性随机数\n\nC++11标准提供了`random_device`类，有可能产生真正的随机数，依赖于具体实现有关。windows平台好像依赖于伪随机生成实现。`std::random_device`是一个非常简单和可靠的随机生成器。该实现是由标准强制执行的，至少在Boost中，它在任何地方都使用相同的代码，这是从原始的`std::mt19937`文件派生而来的。这个代码非常稳定，是跨平台的。您可以非常确信，在任何编译它的平台上，初始化它、从它进行查询等都将编译成类似的代码，并且您将获得类似的性能。\n\n### 随机数生成引擎\nC++11标准预先定义了多个算法的随机数生成引擎\n![](/img/c++11-predefined-random-number-engines.jpg)\n\n随机数生成引擎定义为函数对象类，即实现了`operator()`运算符函数，`operator()`运算符函数无参数，返回随机生成的`unsigned`数，随机数区间位于`[default_randm_engine::min, default_randm_engine::max]`可通过调用随机数引擎对象生成原始随机数。\n\n```c++\nstd::default_random_engine e;  // 种子默认\nfor (int i = 0; i < 10; ++i) {\n  std::cout << e() << std::endl;  // 通过调用e()获得下一个随机数\n}\n```\n\n#### static 引擎和分布\n\n随机数生成引擎在同一初始状态下，会生成相同的随机数序列，这种特性适用于调试情况，若不希望这种情况，可将生成引擎和分布定义为`static`类型，保存其状态，这样多次调用就会不断使用前一个状态生成后续的序列。\n\n注意，一个给定的随机数发生器已知会生成相同的随机数序列。一个函数如果定义了局部的随机数发生器，应该将其（包括引擎和分布对象）定义为 static 的。否则，每次调用函数都会生成相同的序列。\n\n\n```c++\nvoid bad_generator() {\n\tcout << \"bad engine: \";\n\tdefault_random_engine bad_engine;\n\tuniform_int_distribution<> bad_distribution (1, 10);\n\tfor (int i = 0; i < 10; i++) {\n\t\tcout << bad_distribution(bad_engine) << \" \";\n\t}\n\tcout << endl;\n}\n\nvoid good_generator() {\n\tcout << \"good engine: \";\n\tstatic default_random_engine good_engine;\n\tstatic uniform_int_distribution<> good_distribution (1, 10);\n\tfor (int i = 0; i < 10; i++) {\n\t\tcout << good_distribution(good_engine) << \" \";\n\t}\n\tcout << endl;\n}\n\nint main() {\n\tbad_generator();  // 输出 bad engine: 1 2 8 5 6 3 1 7 7 10\n\tbad_generator();  // 输出 bad engine: 1 2 8 5 6 3 1 7 7 10\n\tcout << \"------------------\" << endl;\n\tgood_generator();  // 输出 good engine: 1 2 8 5 6 3 1 7 7 10\n\tgood_generator();  // 输出 good engine: 4 6 9 1 1 6 7 1 4 1\n\n\treturn 0;\n}\n```\n\n\n\n\n\n#### 设置随机数种子\n\n随机数生成引擎在同一初始状态下，会生成相同的随机数序列。可以通过设置种子使得其随机数序列不同。存在两种方法修改随机数种子\n\n* 定义随机数引擎类的构造函数\n* 调用`seed`函数\n\n```c++\ndefault_random_engine e1;\t\t//默认的种子\ndefault_random_engine e2(100);\t        // 构造函数中设定种子\n \ndefault_random_engine e3;  // 默认种子\ne3.seed(100);  // 设置种子\n```\n\n关于种子，旧版本可能使用`time(NULL)`函数来设定种子，但`time`函数精度为秒，重复调用可能会生成相同的种子。这里可利用上文提到的`random_device`类的特性设置种子\n\n```c++\nstd::random_device rd;\nstd::default_random_engine e(rd());\nstd::uniform_int_distribution<> u(5,20);\nfor (size_t i = 0; i < 10; i++) {\n    cout << u(e) << endl;\n}\n```\n\n### 随机分布\n\n类似引擎类型，分布类型也是函数对象类，实现了`operator()`函数分布类型处理随机数生成引擎生成的随机数，使其结果满足某种概率分布。\n\n#### 均匀概率分布(默认类型)\n\n```c++\nstd::default_random_engine e;\nstd::uniform_int_distribution<> u(5, 20);  // 默认类型为unsigned，指定随机数范围区间[5, 10]\nfor (int i = 0; i < 10; ++i) {\n  std::cout << u(e) << std::endl;  // 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。\n}\n```\n\n#### 均匀概率分布(实数类型)\n\n```c++\nstd::default_random_engine e;\nstd::uniform_real_distribution<> u(0, 1);  // 默认类型为double，指定随机数范围区间[0, 1]\nfor (int i = 0; i < 10; ++i) {\n  std::cout << u(e) << std::endl;  // 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。\n}\n```\n\n#### 其他非均匀分布\n\n比如伯努利分布、正态分布、抽样分布、泊松分布等。\n\n![](/img/c++11-predefined-random-number-distributions.jpg)\n\n\n\n## 参考\n\n* https://en.cppreference.com/w/cpp/numeric/random\n* http://www.cplusplus.com/reference/random/\n\n* [Why not just use random_device?](https://stackoverflow.com/questions/39288595/why-not-just-use-random-device)\n* [【C++】c++ 11中的随机数 ——random](https://blog.csdn.net/qq_34784753/article/details/79600809)\n* [C++11随机数发生器 VS rand()](https://blog.csdn.net/songshiMVP1/article/details/47016805)","source":"_posts/C-11伪随机数.md","raw":"---\ntitle: C++11伪随机数\ndate: 2020-07-14 20:11:25\ntags: [C++11, random]\ncategories: [C++11]\nindex_img: img/c++11伪随机数.jpg\nbanner_img: img/c++11伪随机数.jpg\n---\n# C++11 伪随机数\n\n## C风格伪随机数生成\n旧版本C/C++使用`rand()`函数生成区间在`[0, RAND_MAX]`的伪随机数，其存在多种问题：无法满足多种应用程序对不同区间范围、不同数值类型或不同分布的需求。而编程人员在编写程序为了解决这些问题通常会转换 rand 生成的随机数的范围、类型或者是分布时，常常会引入非随机性。\n\n## C++11新标准\nC++11新标准中提供了新的随机数生成库`<random>`来解决上述问题，`<random>`库主要通过`generator`和`distribution`的协作来生成随机数\n\n* random_device类：可能产生真正的随机数，但依具体实现\n\n* random number engines(随机数生成引擎类): 负责生成原始随机数\n* random number distributions(随机数生成分布类): 使得生成的随机数满足统计学中的概率分布\nSTL预先指定了一系列的生成引擎，并且提供一个default_random_engine。default_random_engine会使用某个预定义的引擎，且不同编译器、不同平台的实现可能不同。\n\n### 非确定性随机数\n\nC++11标准提供了`random_device`类，有可能产生真正的随机数，依赖于具体实现有关。windows平台好像依赖于伪随机生成实现。`std::random_device`是一个非常简单和可靠的随机生成器。该实现是由标准强制执行的，至少在Boost中，它在任何地方都使用相同的代码，这是从原始的`std::mt19937`文件派生而来的。这个代码非常稳定，是跨平台的。您可以非常确信，在任何编译它的平台上，初始化它、从它进行查询等都将编译成类似的代码，并且您将获得类似的性能。\n\n### 随机数生成引擎\nC++11标准预先定义了多个算法的随机数生成引擎\n![](/img/c++11-predefined-random-number-engines.jpg)\n\n随机数生成引擎定义为函数对象类，即实现了`operator()`运算符函数，`operator()`运算符函数无参数，返回随机生成的`unsigned`数，随机数区间位于`[default_randm_engine::min, default_randm_engine::max]`可通过调用随机数引擎对象生成原始随机数。\n\n```c++\nstd::default_random_engine e;  // 种子默认\nfor (int i = 0; i < 10; ++i) {\n  std::cout << e() << std::endl;  // 通过调用e()获得下一个随机数\n}\n```\n\n#### static 引擎和分布\n\n随机数生成引擎在同一初始状态下，会生成相同的随机数序列，这种特性适用于调试情况，若不希望这种情况，可将生成引擎和分布定义为`static`类型，保存其状态，这样多次调用就会不断使用前一个状态生成后续的序列。\n\n注意，一个给定的随机数发生器已知会生成相同的随机数序列。一个函数如果定义了局部的随机数发生器，应该将其（包括引擎和分布对象）定义为 static 的。否则，每次调用函数都会生成相同的序列。\n\n\n```c++\nvoid bad_generator() {\n\tcout << \"bad engine: \";\n\tdefault_random_engine bad_engine;\n\tuniform_int_distribution<> bad_distribution (1, 10);\n\tfor (int i = 0; i < 10; i++) {\n\t\tcout << bad_distribution(bad_engine) << \" \";\n\t}\n\tcout << endl;\n}\n\nvoid good_generator() {\n\tcout << \"good engine: \";\n\tstatic default_random_engine good_engine;\n\tstatic uniform_int_distribution<> good_distribution (1, 10);\n\tfor (int i = 0; i < 10; i++) {\n\t\tcout << good_distribution(good_engine) << \" \";\n\t}\n\tcout << endl;\n}\n\nint main() {\n\tbad_generator();  // 输出 bad engine: 1 2 8 5 6 3 1 7 7 10\n\tbad_generator();  // 输出 bad engine: 1 2 8 5 6 3 1 7 7 10\n\tcout << \"------------------\" << endl;\n\tgood_generator();  // 输出 good engine: 1 2 8 5 6 3 1 7 7 10\n\tgood_generator();  // 输出 good engine: 4 6 9 1 1 6 7 1 4 1\n\n\treturn 0;\n}\n```\n\n\n\n\n\n#### 设置随机数种子\n\n随机数生成引擎在同一初始状态下，会生成相同的随机数序列。可以通过设置种子使得其随机数序列不同。存在两种方法修改随机数种子\n\n* 定义随机数引擎类的构造函数\n* 调用`seed`函数\n\n```c++\ndefault_random_engine e1;\t\t//默认的种子\ndefault_random_engine e2(100);\t        // 构造函数中设定种子\n \ndefault_random_engine e3;  // 默认种子\ne3.seed(100);  // 设置种子\n```\n\n关于种子，旧版本可能使用`time(NULL)`函数来设定种子，但`time`函数精度为秒，重复调用可能会生成相同的种子。这里可利用上文提到的`random_device`类的特性设置种子\n\n```c++\nstd::random_device rd;\nstd::default_random_engine e(rd());\nstd::uniform_int_distribution<> u(5,20);\nfor (size_t i = 0; i < 10; i++) {\n    cout << u(e) << endl;\n}\n```\n\n### 随机分布\n\n类似引擎类型，分布类型也是函数对象类，实现了`operator()`函数分布类型处理随机数生成引擎生成的随机数，使其结果满足某种概率分布。\n\n#### 均匀概率分布(默认类型)\n\n```c++\nstd::default_random_engine e;\nstd::uniform_int_distribution<> u(5, 20);  // 默认类型为unsigned，指定随机数范围区间[5, 10]\nfor (int i = 0; i < 10; ++i) {\n  std::cout << u(e) << std::endl;  // 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。\n}\n```\n\n#### 均匀概率分布(实数类型)\n\n```c++\nstd::default_random_engine e;\nstd::uniform_real_distribution<> u(0, 1);  // 默认类型为double，指定随机数范围区间[0, 1]\nfor (int i = 0; i < 10; ++i) {\n  std::cout << u(e) << std::endl;  // 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。\n}\n```\n\n#### 其他非均匀分布\n\n比如伯努利分布、正态分布、抽样分布、泊松分布等。\n\n![](/img/c++11-predefined-random-number-distributions.jpg)\n\n\n\n## 参考\n\n* https://en.cppreference.com/w/cpp/numeric/random\n* http://www.cplusplus.com/reference/random/\n\n* [Why not just use random_device?](https://stackoverflow.com/questions/39288595/why-not-just-use-random-device)\n* [【C++】c++ 11中的随机数 ——random](https://blog.csdn.net/qq_34784753/article/details/79600809)\n* [C++11随机数发生器 VS rand()](https://blog.csdn.net/songshiMVP1/article/details/47016805)","slug":"C-11伪随机数","published":1,"updated":"2020-07-15T02:40:24.033Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke9etpdq0000zdr97mkae6pa","content":"<h1 id=\"C-11-伪随机数\"><a href=\"#C-11-伪随机数\" class=\"headerlink\" title=\"C++11 伪随机数\"></a>C++11 伪随机数</h1><h2 id=\"C风格伪随机数生成\"><a href=\"#C风格伪随机数生成\" class=\"headerlink\" title=\"C风格伪随机数生成\"></a>C风格伪随机数生成</h2><p>旧版本C/C++使用<code>rand()</code>函数生成区间在<code>[0, RAND_MAX]</code>的伪随机数，其存在多种问题：无法满足多种应用程序对不同区间范围、不同数值类型或不同分布的需求。而编程人员在编写程序为了解决这些问题通常会转换 rand 生成的随机数的范围、类型或者是分布时，常常会引入非随机性。</p>\n<h2 id=\"C-11新标准\"><a href=\"#C-11新标准\" class=\"headerlink\" title=\"C++11新标准\"></a>C++11新标准</h2><p>C++11新标准中提供了新的随机数生成库<code>&lt;random&gt;</code>来解决上述问题，<code>&lt;random&gt;</code>库主要通过<code>generator</code>和<code>distribution</code>的协作来生成随机数</p>\n<ul>\n<li><p>random_device类：可能产生真正的随机数，但依具体实现</p>\n</li>\n<li><p>random number engines(随机数生成引擎类): 负责生成原始随机数</p>\n</li>\n<li><p>random number distributions(随机数生成分布类): 使得生成的随机数满足统计学中的概率分布<br>STL预先指定了一系列的生成引擎，并且提供一个default_random_engine。default_random_engine会使用某个预定义的引擎，且不同编译器、不同平台的实现可能不同。</p>\n</li>\n</ul>\n<h3 id=\"非确定性随机数\"><a href=\"#非确定性随机数\" class=\"headerlink\" title=\"非确定性随机数\"></a>非确定性随机数</h3><p>C++11标准提供了<code>random_device</code>类，有可能产生真正的随机数，依赖于具体实现有关。windows平台好像依赖于伪随机生成实现。<code>std::random_device</code>是一个非常简单和可靠的随机生成器。该实现是由标准强制执行的，至少在Boost中，它在任何地方都使用相同的代码，这是从原始的<code>std::mt19937</code>文件派生而来的。这个代码非常稳定，是跨平台的。您可以非常确信，在任何编译它的平台上，初始化它、从它进行查询等都将编译成类似的代码，并且您将获得类似的性能。</p>\n<h3 id=\"随机数生成引擎\"><a href=\"#随机数生成引擎\" class=\"headerlink\" title=\"随机数生成引擎\"></a>随机数生成引擎</h3><p>C++11标准预先定义了多个算法的随机数生成引擎<br><img src=\"/img/c++11-predefined-random-number-engines.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<p>随机数生成引擎定义为函数对象类，即实现了<code>operator()</code>运算符函数，<code>operator()</code>运算符函数无参数，返回随机生成的<code>unsigned</code>数，随机数区间位于<code>[default_randm_engine::min, default_randm_engine::max]</code>可通过调用随机数引擎对象生成原始随机数。</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;  <span class=\"hljs-comment\">// 种子默认</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; e() &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 通过调用e()获得下一个随机数</span>\n&#125;</code></pre>\n\n<h4 id=\"static-引擎和分布\"><a href=\"#static-引擎和分布\" class=\"headerlink\" title=\"static 引擎和分布\"></a>static 引擎和分布</h4><p>随机数生成引擎在同一初始状态下，会生成相同的随机数序列，这种特性适用于调试情况，若不希望这种情况，可将生成引擎和分布定义为<code>static</code>类型，保存其状态，这样多次调用就会不断使用前一个状态生成后续的序列。</p>\n<p>注意，一个给定的随机数发生器已知会生成相同的随机数序列。一个函数如果定义了局部的随机数发生器，应该将其（包括引擎和分布对象）定义为 static 的。否则，每次调用函数都会生成相同的序列。</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">bad_generator</span><span class=\"hljs-params\">()</span> </span>&#123;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"bad engine: \"</span>;\n\tdefault_random_engine bad_engine;\n\t<span class=\"hljs-function\">uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">bad_distribution</span> <span class=\"hljs-params\">(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)</span></span>;\n\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n\t\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; bad_distribution(bad_engine) &lt;&lt; <span class=\"hljs-string\">\" \"</span>;\n\t&#125;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">good_generator</span><span class=\"hljs-params\">()</span> </span>&#123;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"good engine: \"</span>;\n\t<span class=\"hljs-keyword\">static</span> default_random_engine good_engine;\n\t<span class=\"hljs-function\"><span class=\"hljs-keyword\">static</span> uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">good_distribution</span> <span class=\"hljs-params\">(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)</span></span>;\n\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n\t\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; good_distribution(good_engine) &lt;&lt; <span class=\"hljs-string\">\" \"</span>;\n\t&#125;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">int</span> <span class=\"hljs-title\">main</span><span class=\"hljs-params\">()</span> </span>&#123;\n\tbad_generator();  <span class=\"hljs-comment\">// 输出 bad engine: 1 2 8 5 6 3 1 7 7 10</span>\n\tbad_generator();  <span class=\"hljs-comment\">// 输出 bad engine: 1 2 8 5 6 3 1 7 7 10</span>\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"------------------\"</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n\tgood_generator();  <span class=\"hljs-comment\">// 输出 good engine: 1 2 8 5 6 3 1 7 7 10</span>\n\tgood_generator();  <span class=\"hljs-comment\">// 输出 good engine: 4 6 9 1 1 6 7 1 4 1</span>\n\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0</span>;\n&#125;</code></pre>\n\n\n\n\n\n<h4 id=\"设置随机数种子\"><a href=\"#设置随机数种子\" class=\"headerlink\" title=\"设置随机数种子\"></a>设置随机数种子</h4><p>随机数生成引擎在同一初始状态下，会生成相同的随机数序列。可以通过设置种子使得其随机数序列不同。存在两种方法修改随机数种子</p>\n<ul>\n<li>定义随机数引擎类的构造函数</li>\n<li>调用<code>seed</code>函数</li>\n</ul>\n<pre><code class=\"hljs c++\">default_random_engine e1;\t\t<span class=\"hljs-comment\">//默认的种子</span>\n<span class=\"hljs-function\">default_random_engine <span class=\"hljs-title\">e2</span><span class=\"hljs-params\">(<span class=\"hljs-number\">100</span>)</span></span>;\t        <span class=\"hljs-comment\">// 构造函数中设定种子</span>\n \ndefault_random_engine e3;  <span class=\"hljs-comment\">// 默认种子</span>\ne3.seed(<span class=\"hljs-number\">100</span>);  <span class=\"hljs-comment\">// 设置种子</span></code></pre>\n\n<p>关于种子，旧版本可能使用<code>time(NULL)</code>函数来设定种子，但<code>time</code>函数精度为秒，重复调用可能会生成相同的种子。这里可利用上文提到的<code>random_device</code>类的特性设置种子</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::random_device rd;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::default_random_engine <span class=\"hljs-title\">e</span><span class=\"hljs-params\">(rd())</span></span>;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">20</span>)</span></span>;\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">size_t</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n    <span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;</code></pre>\n\n<h3 id=\"随机分布\"><a href=\"#随机分布\" class=\"headerlink\" title=\"随机分布\"></a>随机分布</h3><p>类似引擎类型，分布类型也是函数对象类，实现了<code>operator()</code>函数分布类型处理随机数生成引擎生成的随机数，使其结果满足某种概率分布。</p>\n<h4 id=\"均匀概率分布-默认类型\"><a href=\"#均匀概率分布-默认类型\" class=\"headerlink\" title=\"均匀概率分布(默认类型)\"></a>均匀概率分布(默认类型)</h4><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">20</span>)</span></span>;  <span class=\"hljs-comment\">// 默认类型为unsigned，指定随机数范围区间[5, 10]</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。</span>\n&#125;</code></pre>\n\n<h4 id=\"均匀概率分布-实数类型\"><a href=\"#均匀概率分布-实数类型\" class=\"headerlink\" title=\"均匀概率分布(实数类型)\"></a>均匀概率分布(实数类型)</h4><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_real_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>)</span></span>;  <span class=\"hljs-comment\">// 默认类型为double，指定随机数范围区间[0, 1]</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。</span>\n&#125;</code></pre>\n\n<h4 id=\"其他非均匀分布\"><a href=\"#其他非均匀分布\" class=\"headerlink\" title=\"其他非均匀分布\"></a>其他非均匀分布</h4><p>比如伯努利分布、正态分布、抽样分布、泊松分布等。</p>\n<p><img src=\"/img/c++11-predefined-random-number-distributions.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><p><a href=\"https://en.cppreference.com/w/cpp/numeric/random\" target=\"_blank\" rel=\"noopener\">https://en.cppreference.com/w/cpp/numeric/random</a></p>\n</li>\n<li><p><a href=\"http://www.cplusplus.com/reference/random/\" target=\"_blank\" rel=\"noopener\">http://www.cplusplus.com/reference/random/</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/questions/39288595/why-not-just-use-random-device\" target=\"_blank\" rel=\"noopener\">Why not just use random_device?</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/qq_34784753/article/details/79600809\" target=\"_blank\" rel=\"noopener\">【C++】c++ 11中的随机数 ——random</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/songshiMVP1/article/details/47016805\" target=\"_blank\" rel=\"noopener\">C++11随机数发生器 VS rand()</a></p>\n</li>\n</ul>\n","site":{"data":{"_config":{"favicon":"/img/favicon.png","apple_touch_icon":"/img/favicon.png","title_join_string":" - ","force_https":false,"highlight":{"enable":true,"style":"Github Gist","bg_color":false,"copy_btn":true},"fun_features":{"typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"mouse_click":{"enable":false,"style":"values"}},"color":{"body_bg_color":"#eee","navbar_bg_color":"#2f4154","navbar_text_color":"white","text_color":"#3c4858","sec_text_color":"#718096","post_text_color":"#2c3e50","post_heading_color":"#1a202c","link_color":"#3c4858","link_hover_color":"#1abc9c","link_hover_bg_color":"#f8f9fa","board_color":"#fff"},"font":{"font_size":"16px","font_family":null,"code_font_size":"85%"},"custom_js":null,"custom_css":null,"custom_html":"","web_analytics":{"enable":false,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"tajs":null,"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"lazyload":{"enable":true,"onlypost":false},"daovoice":{"enable":false,"appid":""},"aplayer":{"enable":false,"autoplay":false,"loop":"all","order":"random","theme":"#b7daff","songs":[{"name":"name","artist":"artist","url":"/songs/test.mp3","cover":"/img/cover.jpg"},{"name":"name","artist":"artist","url":"https://...url.mp3","cover":"https://...cover.jpg"}]},"version":{"check":false},"navbar":{"blog_title":"Fluid","ground_glass":{"enable":false,"px":3,"alpha":0.7},"menu":[{"key":"home","link":"/","icon":"iconfont icon-home-fill"},{"key":"archive","link":"/archives/","icon":"iconfont icon-archive-fill"},{"key":"category","link":"/categories/","icon":"iconfont icon-category-fill"},{"key":"tag","link":"/tags/","icon":"iconfont icon-tags-fill"},{"key":"about","link":"/about/","icon":"iconfont icon-user-fill"}]},"search":{"enable":true,"path":"/local-search.xml","generate_path":"/local-search.xml","field":"post","content":true},"scroll_down_arrow":{"enable":true,"banner_height_limit":90,"scroll_after_turning_page":true},"banner_parallax":true,"footer":{"statistics":{"enable":false,"source":"busuanzi","pv_format":"总访问量 {} 次","uv_format":"总访客数 {} 人"},"beian":{"enable":false,"icp_text":"京ICP证123456号","police_text":"京公网安备12345678号","police_code":12345678,"police_icon":"/img/police_beian.png"}},"scroll_top_arrow":{"enable":true},"index":{"banner_img":"/img/default.png","banner_img_height":100,"banner_mask_alpha":0.3,"post_default_img":"","slogan":{"enable":true,"text":"An elegant Material-Design theme for Hexo"},"auto_excerpt":{"enable":true},"post_url_target":"_self","post_meta":{"date":true,"category":true,"tag":true}},"page":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3},"post":{"banner_img":"/img/default.png","banner_img_height":70,"banner_mask_alpha":0.3,"meta":{"date":{"enable":true,"format":"LL a"},"wordcount":{"enable":true,"format":"{} 字"},"min2read":{"enable":true,"format":"{} 分钟","words":100},"views":{"enable":false,"source":"busuanzi","format":"{} 次"}},"updated":{"enable":false,"content":"本文最后更新于：","relative":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"copyright":{"enable":true,"content":"本博客所有文章除特别声明外，均采用 <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.zh\" rel=\"nofollow noopener\">CC BY-SA 4.0 协议</a> ，转载请注明出处！"},"prev_next":{"enable":true},"custom":{"enable":false,"content":"<img src=\"https://octodex.github.com/images/jetpacktocat.png\" class=\"rounded mx-auto d-block mt-5\" style=\"width:150px; height:150px;\">"},"image_zoom":{"enable":true},"footnote":{"enable":true,"header":""},"math":{"enable":false,"specific":false,"engine":"mathjax"},"mermaid":{"enable":false,"specific":false,"options":{"theme":"default"}},"comments":{"enable":false,"type":"disqus"}},"utterances":{"repo":null,"issue_term":null,"label":"utterances","theme":"github-light","crossorigin":"anonymous"},"disqus":{"shortname":null,"disqusjs":false,"apikey":null},"gitalk":{"clientID":null,"clientSecret":null,"repo":null,"owner":null,"admin":null,"id":"location.pathname","language":"zh-CN","labels":"['Gitalk']","perPage":15,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true},"valine":{"appid":null,"appkey":null,"placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":null},"changyan":{"appid":"","appkey":""},"livere":{"uid":""},"archive":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null},"category":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"post_limit":10,"order_by":"-length"},"tag":{"banner_img":"/img/default.png","banner_img_height":80,"banner_mask_alpha":0.3,"subtitle":null,"tagcloud":{"min_font":15,"max_font":30,"unit":"px","start_color":"#BBBBEE","end_color":"#337ab7"}},"about":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"avatar":"/img/avatar.png","name":"myname","introduce":"一句简短的介绍","icons":[{"class":"iconfont icon-github-fill","link":"https://github.com","tip":"GitHub"},{"class":"iconfont icon-douban-fill","link":"https://douban.com","tip":"豆瓣"},{"class":"iconfont icon-wechat-fill","qrcode":"/img/favicon.png"}]},"page404":{"banner_img":"/img/default.png","banner_img_height":85,"banner_mask_alpha":0.3,"subtitle":"Page not found"},"links":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"items":[{"title":"Fluid Docs","intro":"主题使用指南","link":"https://hexo.fluid-dev.com/docs/","image":"/img/favicon.png"},{"title":"Fluid Repo","intro":"主题 GitHub 仓库","link":"https://github.com/fluid-dev/hexo-theme-fluid","image":"/img/favicon.png"},{"title":"Fluid Example","intro":"主题操作示例","link":"https://hexo.fluid-dev.com/docs/example/","image":"/img/favicon.png"}]}}}},"excerpt":"","more":"<h1 id=\"C-11-伪随机数\"><a href=\"#C-11-伪随机数\" class=\"headerlink\" title=\"C++11 伪随机数\"></a>C++11 伪随机数</h1><h2 id=\"C风格伪随机数生成\"><a href=\"#C风格伪随机数生成\" class=\"headerlink\" title=\"C风格伪随机数生成\"></a>C风格伪随机数生成</h2><p>旧版本C/C++使用<code>rand()</code>函数生成区间在<code>[0, RAND_MAX]</code>的伪随机数，其存在多种问题：无法满足多种应用程序对不同区间范围、不同数值类型或不同分布的需求。而编程人员在编写程序为了解决这些问题通常会转换 rand 生成的随机数的范围、类型或者是分布时，常常会引入非随机性。</p>\n<h2 id=\"C-11新标准\"><a href=\"#C-11新标准\" class=\"headerlink\" title=\"C++11新标准\"></a>C++11新标准</h2><p>C++11新标准中提供了新的随机数生成库<code>&lt;random&gt;</code>来解决上述问题，<code>&lt;random&gt;</code>库主要通过<code>generator</code>和<code>distribution</code>的协作来生成随机数</p>\n<ul>\n<li><p>random_device类：可能产生真正的随机数，但依具体实现</p>\n</li>\n<li><p>random number engines(随机数生成引擎类): 负责生成原始随机数</p>\n</li>\n<li><p>random number distributions(随机数生成分布类): 使得生成的随机数满足统计学中的概率分布<br>STL预先指定了一系列的生成引擎，并且提供一个default_random_engine。default_random_engine会使用某个预定义的引擎，且不同编译器、不同平台的实现可能不同。</p>\n</li>\n</ul>\n<h3 id=\"非确定性随机数\"><a href=\"#非确定性随机数\" class=\"headerlink\" title=\"非确定性随机数\"></a>非确定性随机数</h3><p>C++11标准提供了<code>random_device</code>类，有可能产生真正的随机数，依赖于具体实现有关。windows平台好像依赖于伪随机生成实现。<code>std::random_device</code>是一个非常简单和可靠的随机生成器。该实现是由标准强制执行的，至少在Boost中，它在任何地方都使用相同的代码，这是从原始的<code>std::mt19937</code>文件派生而来的。这个代码非常稳定，是跨平台的。您可以非常确信，在任何编译它的平台上，初始化它、从它进行查询等都将编译成类似的代码，并且您将获得类似的性能。</p>\n<h3 id=\"随机数生成引擎\"><a href=\"#随机数生成引擎\" class=\"headerlink\" title=\"随机数生成引擎\"></a>随机数生成引擎</h3><p>C++11标准预先定义了多个算法的随机数生成引擎<br><img src=\"/img/c++11-predefined-random-number-engines.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<p>随机数生成引擎定义为函数对象类，即实现了<code>operator()</code>运算符函数，<code>operator()</code>运算符函数无参数，返回随机生成的<code>unsigned</code>数，随机数区间位于<code>[default_randm_engine::min, default_randm_engine::max]</code>可通过调用随机数引擎对象生成原始随机数。</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;  <span class=\"hljs-comment\">// 种子默认</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; e() &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 通过调用e()获得下一个随机数</span>\n&#125;</code></pre>\n\n<h4 id=\"static-引擎和分布\"><a href=\"#static-引擎和分布\" class=\"headerlink\" title=\"static 引擎和分布\"></a>static 引擎和分布</h4><p>随机数生成引擎在同一初始状态下，会生成相同的随机数序列，这种特性适用于调试情况，若不希望这种情况，可将生成引擎和分布定义为<code>static</code>类型，保存其状态，这样多次调用就会不断使用前一个状态生成后续的序列。</p>\n<p>注意，一个给定的随机数发生器已知会生成相同的随机数序列。一个函数如果定义了局部的随机数发生器，应该将其（包括引擎和分布对象）定义为 static 的。否则，每次调用函数都会生成相同的序列。</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">bad_generator</span><span class=\"hljs-params\">()</span> </span>&#123;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"bad engine: \"</span>;\n\tdefault_random_engine bad_engine;\n\t<span class=\"hljs-function\">uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">bad_distribution</span> <span class=\"hljs-params\">(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)</span></span>;\n\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n\t\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; bad_distribution(bad_engine) &lt;&lt; <span class=\"hljs-string\">\" \"</span>;\n\t&#125;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">void</span> <span class=\"hljs-title\">good_generator</span><span class=\"hljs-params\">()</span> </span>&#123;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"good engine: \"</span>;\n\t<span class=\"hljs-keyword\">static</span> default_random_engine good_engine;\n\t<span class=\"hljs-function\"><span class=\"hljs-keyword\">static</span> uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">good_distribution</span> <span class=\"hljs-params\">(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">10</span>)</span></span>;\n\t<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n\t\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; good_distribution(good_engine) &lt;&lt; <span class=\"hljs-string\">\" \"</span>;\n\t&#125;\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;\n\n<span class=\"hljs-function\"><span class=\"hljs-keyword\">int</span> <span class=\"hljs-title\">main</span><span class=\"hljs-params\">()</span> </span>&#123;\n\tbad_generator();  <span class=\"hljs-comment\">// 输出 bad engine: 1 2 8 5 6 3 1 7 7 10</span>\n\tbad_generator();  <span class=\"hljs-comment\">// 输出 bad engine: 1 2 8 5 6 3 1 7 7 10</span>\n\t<span class=\"hljs-built_in\">cout</span> &lt;&lt; <span class=\"hljs-string\">\"------------------\"</span> &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n\tgood_generator();  <span class=\"hljs-comment\">// 输出 good engine: 1 2 8 5 6 3 1 7 7 10</span>\n\tgood_generator();  <span class=\"hljs-comment\">// 输出 good engine: 4 6 9 1 1 6 7 1 4 1</span>\n\n\t<span class=\"hljs-keyword\">return</span> <span class=\"hljs-number\">0</span>;\n&#125;</code></pre>\n\n\n\n\n\n<h4 id=\"设置随机数种子\"><a href=\"#设置随机数种子\" class=\"headerlink\" title=\"设置随机数种子\"></a>设置随机数种子</h4><p>随机数生成引擎在同一初始状态下，会生成相同的随机数序列。可以通过设置种子使得其随机数序列不同。存在两种方法修改随机数种子</p>\n<ul>\n<li>定义随机数引擎类的构造函数</li>\n<li>调用<code>seed</code>函数</li>\n</ul>\n<pre><code class=\"hljs c++\">default_random_engine e1;\t\t<span class=\"hljs-comment\">//默认的种子</span>\n<span class=\"hljs-function\">default_random_engine <span class=\"hljs-title\">e2</span><span class=\"hljs-params\">(<span class=\"hljs-number\">100</span>)</span></span>;\t        <span class=\"hljs-comment\">// 构造函数中设定种子</span>\n \ndefault_random_engine e3;  <span class=\"hljs-comment\">// 默认种子</span>\ne3.seed(<span class=\"hljs-number\">100</span>);  <span class=\"hljs-comment\">// 设置种子</span></code></pre>\n\n<p>关于种子，旧版本可能使用<code>time(NULL)</code>函数来设定种子，但<code>time</code>函数精度为秒，重复调用可能会生成相同的种子。这里可利用上文提到的<code>random_device</code>类的特性设置种子</p>\n<pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::random_device rd;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::default_random_engine <span class=\"hljs-title\">e</span><span class=\"hljs-params\">(rd())</span></span>;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">20</span>)</span></span>;\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">size_t</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; i++) &#123;\n    <span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">endl</span>;\n&#125;</code></pre>\n\n<h3 id=\"随机分布\"><a href=\"#随机分布\" class=\"headerlink\" title=\"随机分布\"></a>随机分布</h3><p>类似引擎类型，分布类型也是函数对象类，实现了<code>operator()</code>函数分布类型处理随机数生成引擎生成的随机数，使其结果满足某种概率分布。</p>\n<h4 id=\"均匀概率分布-默认类型\"><a href=\"#均匀概率分布-默认类型\" class=\"headerlink\" title=\"均匀概率分布(默认类型)\"></a>均匀概率分布(默认类型)</h4><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_int_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">20</span>)</span></span>;  <span class=\"hljs-comment\">// 默认类型为unsigned，指定随机数范围区间[5, 10]</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。</span>\n&#125;</code></pre>\n\n<h4 id=\"均匀概率分布-实数类型\"><a href=\"#均匀概率分布-实数类型\" class=\"headerlink\" title=\"均匀概率分布(实数类型)\"></a>均匀概率分布(实数类型)</h4><pre><code class=\"hljs c++\"><span class=\"hljs-built_in\">std</span>::default_random_engine e;\n<span class=\"hljs-function\"><span class=\"hljs-built_in\">std</span>::uniform_real_distribution&lt;&gt; <span class=\"hljs-title\">u</span><span class=\"hljs-params\">(<span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">1</span>)</span></span>;  <span class=\"hljs-comment\">// 默认类型为double，指定随机数范围区间[0, 1]</span>\n<span class=\"hljs-keyword\">for</span> (<span class=\"hljs-keyword\">int</span> i = <span class=\"hljs-number\">0</span>; i &lt; <span class=\"hljs-number\">10</span>; ++i) &#123;\n  <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">cout</span> &lt;&lt; u(e) &lt;&lt; <span class=\"hljs-built_in\">std</span>::<span class=\"hljs-built_in\">endl</span>;  <span class=\"hljs-comment\">// 传递引擎对象给分布对象，因为某些分布可能需要调用引擎多次才能产生一个值。</span>\n&#125;</code></pre>\n\n<h4 id=\"其他非均匀分布\"><a href=\"#其他非均匀分布\" class=\"headerlink\" title=\"其他非均匀分布\"></a>其他非均匀分布</h4><p>比如伯努利分布、正态分布、抽样分布、泊松分布等。</p>\n<p><img src=\"/img/c++11-predefined-random-number-distributions.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<h2 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h2><ul>\n<li><p><a href=\"https://en.cppreference.com/w/cpp/numeric/random\" target=\"_blank\" rel=\"noopener\">https://en.cppreference.com/w/cpp/numeric/random</a></p>\n</li>\n<li><p><a href=\"http://www.cplusplus.com/reference/random/\" target=\"_blank\" rel=\"noopener\">http://www.cplusplus.com/reference/random/</a></p>\n</li>\n<li><p><a href=\"https://stackoverflow.com/questions/39288595/why-not-just-use-random-device\" target=\"_blank\" rel=\"noopener\">Why not just use random_device?</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/qq_34784753/article/details/79600809\" target=\"_blank\" rel=\"noopener\">【C++】c++ 11中的随机数 ——random</a></p>\n</li>\n<li><p><a href=\"https://blog.csdn.net/songshiMVP1/article/details/47016805\" target=\"_blank\" rel=\"noopener\">C++11随机数发生器 VS rand()</a></p>\n</li>\n</ul>\n"},{"title":"A Neural Probabilistic Language Model","date":"2020-07-19T09:22:22.000Z","caetgories":["Deep Learning"],"index_img":"img/A-Neural-Probabilistic-Language-Model/figure1.jpg","banner_img":"img/A-Neural-Probabilistic-Language-Model/figure1.jpg","_content":"\n# A Neural Probabilistic Language Model\n\n​\t本[paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。\n\n## Abstract\n​\t统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，导致目标非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列（这样导致高纬度下的数据系数会导致很多单词序列的概率为0）。<span style=\"color:red;\">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）根据其分布式表示得到的单词序列的概率函数。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。\n\n## Introduction\n\n​\t语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。\n\n​\t统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\\hat{P}(w^T_1) = \\prod^T_{t=1} \\hat{P}(w_t|w^{t-1}_1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w_{i+1}, ..., w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。\n\n​\t当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个上下文，`n-gram`模型构建了上下文和下一个单词的条件概率表，例如前$n-1$个单词的组合：$$\\hat{p}(w_t|w^{t-1}_1) \\approx \\hat{p}(w_t|w^{t-1}_{t-n+1})$$，我们仅考虑在训练预料中真实存在或出现频率足够的连续单词的组合。但是若某个包含`n`个单词的单词序列未在训练预料中出现过怎么办？对于这种情况，并不能将其概率赋为$0$，因为这样的单词序列是很有可能会在更大的上下文中出现的。一个简单的方法是使用更小的上下文大小来预测其概率，正如论文back-off trigram model(Katz, 1987)以及smoothed(or interpolated) trigram models(Jelinek and Mercer, 1980)所示。因此，在这样的模型中，如何从训练预料中见过的单词序列中获得对新的单词序列的泛化能力呢？一种对此类模型获得泛化能力的理解是将此类interpolated或back-off n-gram模型思考为生成模型。最终，新的单词序列可通过“粘合(gluing)”训练语料中常见的较短、重叠的长度为1,2...到n个单词的序列块生成。在这种back-off或interpolated n-gram模型中，得到下一个单词的规则是隐式的。研究者在使用$n=3$，即trigrams中得到了state-of-art 结果，并且关于结合技巧来得到实质性改进的详细请看Goodman(2001). 显然，距离预测单词越近的前n个单词所包含的信息更多，但上述方法至少有两个方面值得改进，且本文也着重关注：1. 未考虑超过1或者2的更大的上下文；2. 未考虑单词间的相似度。比如，若训练语料中存在句子\"The cat is walking in the bedroom\"，则类似句子\"A dog was running in a room\"应该具有更高可能性，因为\"dog\"和\"cat\"(\"the\"和\"a\", \"room\" 和\"bedroom\" 等等)在语义和语法上很相似。\n\n**Contributions**\n\n1. 提出单词的分布式表示，解决维度灾难问题\n2. 解决单词的长距离依赖限制，而ngram模型中一般$n=3$\n3. 词的相似关系，本论文中单词以分布式表示，能够表示单词间的相似性\n\n### 1.1 Fighting the Curse of Dimensionality with Distributed Representations\n\n​\t简而言之，本文提出的方法可归纳为如下几点\n\n1. 将词典中的每个词都关联到一个分布式的*word feature vector*（一个$\\mathbb{R}^m$空间的实值向量）\n2. 单词序列中的联合*probability function*通过单词的特征向量表征\n3. 同时学习*word feature vectors*和*probability function*的参数\n\n​\t单词的特征向量表征单词的不同方面：每个单词都关联到向量空间中的某个点。特征数目（比如m=30,60或者100）远小于词典大小（比如17000）。概率函数表示为在给定之前单词序列时，预测下一个单词的条件概率的乘积（比如实验中在给定先前单词时，采用了多层神经网络预测下一个单词）。概率函数的参数可被迭代调整以达到**maxmize the log-likelihood of training data最大化训练数据的似然函数**或者正则标准，比如增加一个权重衰减惩罚项。最后，每个单词的特征向量都可被学习，当然可采用语义特征的先验知识初始化。\n\n​\t该方法为什么有效呢？在先前例子所示，若*dog*和*cat*的语法和语义相似，并且*(the, a), (bedroom, room), (is, was), (running, walking)*也类似，则很自然的可从**The cat is walking in the bedroom** 生成**A dog was running in a room**，并且类似的**The cat is running in a room**、**A dog is walking in a bedroom**等等等其他各种组合。本文提出的模型是可以泛化的，因为**相似的**单词期望拥有相似的特征向量，并且概率函数是特征向量的*smooth平滑*函数，即特征上的微小变化也只会引起概率的微小变化。因此，训练数据中的上述句子的出现会增加概率，当然不仅仅是上述句子，还包括其在语句空间中组合数目的*邻居*句子（通过特征向量表达的句子）\n\n### 1.2 Relation to Previous Work\n\n略\n\n## 2. A Neural Model\n\n​\t训练集是诸如$w_1 ... w_T$的单词序列，其中单词$w_t \\in V$，而词典$V$是一个巨大但有限的集合。目标是学习一个好模型\n$$\nf(w_t, ..., w_{t-n+1}) = \\hat{p}(w_t|w^{t-1}_1)\n$$\n下面，我们给出概率的几何倒数$\\frac{1}{\\hat{p}(w_t|w^{t-1}_1)}$，也被称为*perplexity困惑度*，也是平均负似然函数的指数。模型的唯一约束在于对于任何的$w^{t-1}_1$， $\\sum^{|V|}_{i=1}f(i, w_{t-1}, ..., w_{t-n+1}) = 1, f > 0$，通过将这些条件概率相乘，得到单词序列的联合概率模型。\n\n​\t我们将函数$f(w_t, ..., w_{t-n+1}=\\hat{p}(w_t|w^{t-1}_1)$分解为两部分\n\n1. 一个映射函数$C$， 它将词典$V$中的任意元素$i$映射为一个实值向量$C(i) \\in \\mathbb{R}^m$。它表示了词典中每个单词的*分布式特征向量 distributed feature vectors*。实际上，$C$是一个无参数的$|V| * m$的矩阵\n\n2. 一个由$C$表达的单词的概率函数：上下文单词的特征向量序列$(C(w_{t-n+1}), ..., C(w_{t-1}))$作为函数$g$的输入，映射到词典$V$中下一个单词$w_t$的条件概率分布。函数$g$的输出是一个向量，其中第$i$-th 元素表示概率$\\hat{p}(w_t = i|w^{t-1}_1)$ ，如图1所示\n   $$\n   f(i, w_{t-1}, ..., w_{t-n+1}) = g(i, C(w_{t-1}), ..., C(w_{t-n+1}))\n   $$\n\n函数$f$是上述两个映射($C$和$g$)的组合，并且$C$对于上下文中的所有单词都是共享参数的。这两部分都与一些参数有关，映射函数$C$的参数就是特征向量本身，表示为一个$|V|*m$的矩阵，其中第$i$行$C(i)$是单词$i$的特征向量。函数$g$可通过一个前向传播网络或RNN或其他参数化函数实现，参数为$\\omega$。整体的参数集合为$\\Theta = (C, \\omega)$\n\n![](/img/A-Neural-Probabilistic-Language-Model/figure1.jpg)\n\n​\t训练过程是寻找最大化训练语料的penalized log-likelihood的参数$\\Theta$, \n$$\nL = \\frac{1}{T}\\sum_t log f(w_t, ..., w_{t-n+1}; \\Theta) + R(\\Theta)\n$$\n其中$R(\\Theta)$是正则化项，比如，在实验中，$R$是一个权重衰减惩罚项，只作用于神经网络的权重和矩阵$C$，不包括偏置项biases\n\n​\t在上述模型中，自由参数(free parameters)的数目只与词汇表大小$V$成线性关系。它也只与阶数$n$成线性关系：若引入更多共享参数结构，则比例因子可降为sub-linear次线性，比如使用time-decay神经网络或rnn（或两种网络的组合形式）\n\n​\t在如下大多数实验中，除了word features mapping外，神经网络只有一个隐藏层，并且可选性的，存在从word features到输出的直接连接。因此，实际上，神经网络有两个隐藏层：1. the shared word features layer C 即共享单词特征向量层$C$， 没有任何非线性能力（无实质作用）2. the ordinay hyperbolic tangent hidden layer即普通双曲正切隐藏层。更准确的讲，神经网络计算如下函数，使用*softmax*输出层，保证概率值为正，且和为1\n$$\n\\hat{p}(w_t|w_{t-1}, ..., w_{t-n+1}) = \\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}\n$$\n其中$y_i$是每个输出单词$i$的未归一化的log-概率，包含参数$b, W, U, d, H$的计算公式如下\n$$\ny = b + Wx + Utanh(d + Hx)\n$$\n其中，双曲正切tanh是element-by-element方式作用，$W$可选为0，表示无直接连接。$x$为word features layer单词特征向量层的激活向量，是矩阵$C$上的输入单词的特征向量的拼接向量\n$$\nx = (C(w_{t-1}), C(w_{t-2}), ..., C(w_{t-n+1}))\n$$\n设$h$为隐藏层单元数目，$m$为每个单词的特征向量的维度数目。若不存在word features到输出的直接连接，则矩阵$W$为0.模型的自由参数是输出层偏置项$b$（包含$|V|$个元素），隐藏层偏置项$d$（包含$h$个元素），隐藏层到输出层的权重$U$（一个$|V| \\times h$的矩阵），word features 到输出的权重$W$（一个$|V| \\times (n-1)m$的矩阵），隐藏层权重$H$（一个$h \\times (n-1)m$的矩阵），以及word features $C$（一个$|V| \\times m$的矩阵）\n$$\n\\Theta = (b, d, W, U, H, C)\n$$\n自由参数的数目是$|V|(1 + nm + h) + h(1 + (n-1)m)$。主导因子是$|V|(nm + h)$，注意在里乱上，若只对权重$W$和$H$而不包含$C$上存在权重衰减，则$W$和$H$会收敛到0，而$C$会继续增长，当然在实际中使用随机梯度上升不会存在这种现象。\n\n​\t在神经网络上执行随机梯度上升算法，则执行第t个单词的迭代更新公式如下\n$$\n\\Theta \\leftarrow \\Theta + \\epsilon \\frac{\\partial \\hat{P}(w_t|w_{t-1}, ..., w_{t-n+1})}{\\partial \\Theta}\n$$\n其中$\\epsilon$表示学习率，注意大量的参数无需更新：对于大量单词$j$的$C(j)$单词特征没有出现在输入窗口中。\n\n\n\n## 3. Parallel Implementation\n\n略\n\n## 4. Experimental Results\n\n​\t对比实验使用包含各种英语文本和书籍的共计1181041个单词的布朗语料库，前800000单词用作训练，接下来200000用作验证集（模型选择、权重衰减、early stopping），剩余181041用作测试。不同单词的数目有47578（包括标点符号、大小写以及文本段落标识符），将频率<=3的稀有词合并为单个词，减小词表大小为$|V|=16383$\n\n​\t第二个实验使用1995到1996年的Associated Press(AP) News。训练集大约为1400万（13994528）单词，验证集约为100万（963138）单词，测试集也约为100万（963071）单词。原始数据共计148721单词（包括标点符号），减少为$|V|=17964$，处理方法是保留常用词（保留标点符号），大写转为小写，数字映射为特殊符号，稀有词及专有词映射为特殊符号\n\n​\t为了训练神经网络，初始学习率设为$\\epsilon_o = 10^{-3}$（在使用多个小数据尝试后），并根据如下规则衰减$\\epsilon_t = \\frac{\\epsilon_o}{1 + rt}$，t表示完成的参数更新次数，r表示参数衰减因子 $r = 10^{-8}$\n\n### 4.1 N-Gram Models\n\n第一个对比模型是采用插值法（interpolated）或平滑法（smoothed）的trigram 模型（Jelinek and Mercer, 1980）。模型的条件概率为\n$$\n\\hat{p}(w_t|w_{t-1}, w_{t-2}) = \\alpha_0(q_t)p_0 + \\alpha_1(q_t)p_1(w_t) + \\alpha_2(q_t)p_2(w_t|w_{t-1}) + \\alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})\n$$\n其中条件权重$\\alpha_i(q_t) \\geq 0, \\sum_i \\alpha_i(q_t) = 1$，$p_0 = 1/|V|$， $p_1(i)$为unigram（单词$i$在训练集中的相对频率），$p_2(i|j)$为bigram（$\\frac{词组(j,i)}{词(j)}$），$p_3(i|j,k)$ 为trigram（$\\frac{词组(k,j,i)}{词(j,i)}$）.对每个离散值$q_t$都有混合权重$\\alpha$，可通过EM算法经过大约5次迭代估算出。\n\n### 4.2 Results\n\n下图为不同模型在困惑度上的表现，可以看到神经网络模型表现最好\n\n![](/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg)\n\n## 5. Conclusion\n\n​\t实验作用在两个语料库中，一个超过100万数据，另一个超过1500万单词，都证明了本方法比state-of-art方法比如smoothed trigram优秀，困惑度低于10%到20%\n\n​\t我们坚信，主要原因在于利用了可学习的分布式表示来解决维度灾难，每个训练语句都存在相当的组合数目的其他邻居语句来训练模型\n\n​\t当然，模型还存在较多可改进之处，比如架构、计算性能、先验知识等，未来重点可放在改进加速技术，以及不增加训练时间的基础上增加容量（处理数亿个单词或更多）。利用时间结构扩展输入窗口以包含整个段落（无需增加参数数目或计算时间）的简单想法是使用time-delay或者rnn。\n\n## 6. 参考\n\n1. [A Neural Probabilistic Language Model](https://www.cnblogs.com/Dream-Fish/p/3950024.html)\n\n1. [NNLM 的 PyTorch 实现](https://wmathor.com/index.php/archives/1442/)\n\n## 7. 代码实现\n\n[DreamExplorerX-nnlm](https://github.com/DreamExplorerX/NLP-Garden/tree/master/1-1.NNLM)\n\n","source":"_posts/A-Neural-Probabilistic-Language-Model.md","raw":"---\ntitle: A Neural Probabilistic Language Model\ndate: 2020-07-19 17:22:22\ntags: [Deep Learning, NLP]\ncaetgories: [Deep Learning]\nindex_img: img/A-Neural-Probabilistic-Language-Model/figure1.jpg\nbanner_img: img/A-Neural-Probabilistic-Language-Model/figure1.jpg\n---\n\n# A Neural Probabilistic Language Model\n\n​\t本[paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。\n\n## Abstract\n​\t统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，导致目标非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列（这样导致高纬度下的数据系数会导致很多单词序列的概率为0）。<span style=\"color:red;\">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）根据其分布式表示得到的单词序列的概率函数。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。\n\n## Introduction\n\n​\t语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。\n\n​\t统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\\hat{P}(w^T_1) = \\prod^T_{t=1} \\hat{P}(w_t|w^{t-1}_1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w_{i+1}, ..., w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。\n\n​\t当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个上下文，`n-gram`模型构建了上下文和下一个单词的条件概率表，例如前$n-1$个单词的组合：$$\\hat{p}(w_t|w^{t-1}_1) \\approx \\hat{p}(w_t|w^{t-1}_{t-n+1})$$，我们仅考虑在训练预料中真实存在或出现频率足够的连续单词的组合。但是若某个包含`n`个单词的单词序列未在训练预料中出现过怎么办？对于这种情况，并不能将其概率赋为$0$，因为这样的单词序列是很有可能会在更大的上下文中出现的。一个简单的方法是使用更小的上下文大小来预测其概率，正如论文back-off trigram model(Katz, 1987)以及smoothed(or interpolated) trigram models(Jelinek and Mercer, 1980)所示。因此，在这样的模型中，如何从训练预料中见过的单词序列中获得对新的单词序列的泛化能力呢？一种对此类模型获得泛化能力的理解是将此类interpolated或back-off n-gram模型思考为生成模型。最终，新的单词序列可通过“粘合(gluing)”训练语料中常见的较短、重叠的长度为1,2...到n个单词的序列块生成。在这种back-off或interpolated n-gram模型中，得到下一个单词的规则是隐式的。研究者在使用$n=3$，即trigrams中得到了state-of-art 结果，并且关于结合技巧来得到实质性改进的详细请看Goodman(2001). 显然，距离预测单词越近的前n个单词所包含的信息更多，但上述方法至少有两个方面值得改进，且本文也着重关注：1. 未考虑超过1或者2的更大的上下文；2. 未考虑单词间的相似度。比如，若训练语料中存在句子\"The cat is walking in the bedroom\"，则类似句子\"A dog was running in a room\"应该具有更高可能性，因为\"dog\"和\"cat\"(\"the\"和\"a\", \"room\" 和\"bedroom\" 等等)在语义和语法上很相似。\n\n**Contributions**\n\n1. 提出单词的分布式表示，解决维度灾难问题\n2. 解决单词的长距离依赖限制，而ngram模型中一般$n=3$\n3. 词的相似关系，本论文中单词以分布式表示，能够表示单词间的相似性\n\n### 1.1 Fighting the Curse of Dimensionality with Distributed Representations\n\n​\t简而言之，本文提出的方法可归纳为如下几点\n\n1. 将词典中的每个词都关联到一个分布式的*word feature vector*（一个$\\mathbb{R}^m$空间的实值向量）\n2. 单词序列中的联合*probability function*通过单词的特征向量表征\n3. 同时学习*word feature vectors*和*probability function*的参数\n\n​\t单词的特征向量表征单词的不同方面：每个单词都关联到向量空间中的某个点。特征数目（比如m=30,60或者100）远小于词典大小（比如17000）。概率函数表示为在给定之前单词序列时，预测下一个单词的条件概率的乘积（比如实验中在给定先前单词时，采用了多层神经网络预测下一个单词）。概率函数的参数可被迭代调整以达到**maxmize the log-likelihood of training data最大化训练数据的似然函数**或者正则标准，比如增加一个权重衰减惩罚项。最后，每个单词的特征向量都可被学习，当然可采用语义特征的先验知识初始化。\n\n​\t该方法为什么有效呢？在先前例子所示，若*dog*和*cat*的语法和语义相似，并且*(the, a), (bedroom, room), (is, was), (running, walking)*也类似，则很自然的可从**The cat is walking in the bedroom** 生成**A dog was running in a room**，并且类似的**The cat is running in a room**、**A dog is walking in a bedroom**等等等其他各种组合。本文提出的模型是可以泛化的，因为**相似的**单词期望拥有相似的特征向量，并且概率函数是特征向量的*smooth平滑*函数，即特征上的微小变化也只会引起概率的微小变化。因此，训练数据中的上述句子的出现会增加概率，当然不仅仅是上述句子，还包括其在语句空间中组合数目的*邻居*句子（通过特征向量表达的句子）\n\n### 1.2 Relation to Previous Work\n\n略\n\n## 2. A Neural Model\n\n​\t训练集是诸如$w_1 ... w_T$的单词序列，其中单词$w_t \\in V$，而词典$V$是一个巨大但有限的集合。目标是学习一个好模型\n$$\nf(w_t, ..., w_{t-n+1}) = \\hat{p}(w_t|w^{t-1}_1)\n$$\n下面，我们给出概率的几何倒数$\\frac{1}{\\hat{p}(w_t|w^{t-1}_1)}$，也被称为*perplexity困惑度*，也是平均负似然函数的指数。模型的唯一约束在于对于任何的$w^{t-1}_1$， $\\sum^{|V|}_{i=1}f(i, w_{t-1}, ..., w_{t-n+1}) = 1, f > 0$，通过将这些条件概率相乘，得到单词序列的联合概率模型。\n\n​\t我们将函数$f(w_t, ..., w_{t-n+1}=\\hat{p}(w_t|w^{t-1}_1)$分解为两部分\n\n1. 一个映射函数$C$， 它将词典$V$中的任意元素$i$映射为一个实值向量$C(i) \\in \\mathbb{R}^m$。它表示了词典中每个单词的*分布式特征向量 distributed feature vectors*。实际上，$C$是一个无参数的$|V| * m$的矩阵\n\n2. 一个由$C$表达的单词的概率函数：上下文单词的特征向量序列$(C(w_{t-n+1}), ..., C(w_{t-1}))$作为函数$g$的输入，映射到词典$V$中下一个单词$w_t$的条件概率分布。函数$g$的输出是一个向量，其中第$i$-th 元素表示概率$\\hat{p}(w_t = i|w^{t-1}_1)$ ，如图1所示\n   $$\n   f(i, w_{t-1}, ..., w_{t-n+1}) = g(i, C(w_{t-1}), ..., C(w_{t-n+1}))\n   $$\n\n函数$f$是上述两个映射($C$和$g$)的组合，并且$C$对于上下文中的所有单词都是共享参数的。这两部分都与一些参数有关，映射函数$C$的参数就是特征向量本身，表示为一个$|V|*m$的矩阵，其中第$i$行$C(i)$是单词$i$的特征向量。函数$g$可通过一个前向传播网络或RNN或其他参数化函数实现，参数为$\\omega$。整体的参数集合为$\\Theta = (C, \\omega)$\n\n![](/img/A-Neural-Probabilistic-Language-Model/figure1.jpg)\n\n​\t训练过程是寻找最大化训练语料的penalized log-likelihood的参数$\\Theta$, \n$$\nL = \\frac{1}{T}\\sum_t log f(w_t, ..., w_{t-n+1}; \\Theta) + R(\\Theta)\n$$\n其中$R(\\Theta)$是正则化项，比如，在实验中，$R$是一个权重衰减惩罚项，只作用于神经网络的权重和矩阵$C$，不包括偏置项biases\n\n​\t在上述模型中，自由参数(free parameters)的数目只与词汇表大小$V$成线性关系。它也只与阶数$n$成线性关系：若引入更多共享参数结构，则比例因子可降为sub-linear次线性，比如使用time-decay神经网络或rnn（或两种网络的组合形式）\n\n​\t在如下大多数实验中，除了word features mapping外，神经网络只有一个隐藏层，并且可选性的，存在从word features到输出的直接连接。因此，实际上，神经网络有两个隐藏层：1. the shared word features layer C 即共享单词特征向量层$C$， 没有任何非线性能力（无实质作用）2. the ordinay hyperbolic tangent hidden layer即普通双曲正切隐藏层。更准确的讲，神经网络计算如下函数，使用*softmax*输出层，保证概率值为正，且和为1\n$$\n\\hat{p}(w_t|w_{t-1}, ..., w_{t-n+1}) = \\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}\n$$\n其中$y_i$是每个输出单词$i$的未归一化的log-概率，包含参数$b, W, U, d, H$的计算公式如下\n$$\ny = b + Wx + Utanh(d + Hx)\n$$\n其中，双曲正切tanh是element-by-element方式作用，$W$可选为0，表示无直接连接。$x$为word features layer单词特征向量层的激活向量，是矩阵$C$上的输入单词的特征向量的拼接向量\n$$\nx = (C(w_{t-1}), C(w_{t-2}), ..., C(w_{t-n+1}))\n$$\n设$h$为隐藏层单元数目，$m$为每个单词的特征向量的维度数目。若不存在word features到输出的直接连接，则矩阵$W$为0.模型的自由参数是输出层偏置项$b$（包含$|V|$个元素），隐藏层偏置项$d$（包含$h$个元素），隐藏层到输出层的权重$U$（一个$|V| \\times h$的矩阵），word features 到输出的权重$W$（一个$|V| \\times (n-1)m$的矩阵），隐藏层权重$H$（一个$h \\times (n-1)m$的矩阵），以及word features $C$（一个$|V| \\times m$的矩阵）\n$$\n\\Theta = (b, d, W, U, H, C)\n$$\n自由参数的数目是$|V|(1 + nm + h) + h(1 + (n-1)m)$。主导因子是$|V|(nm + h)$，注意在里乱上，若只对权重$W$和$H$而不包含$C$上存在权重衰减，则$W$和$H$会收敛到0，而$C$会继续增长，当然在实际中使用随机梯度上升不会存在这种现象。\n\n​\t在神经网络上执行随机梯度上升算法，则执行第t个单词的迭代更新公式如下\n$$\n\\Theta \\leftarrow \\Theta + \\epsilon \\frac{\\partial \\hat{P}(w_t|w_{t-1}, ..., w_{t-n+1})}{\\partial \\Theta}\n$$\n其中$\\epsilon$表示学习率，注意大量的参数无需更新：对于大量单词$j$的$C(j)$单词特征没有出现在输入窗口中。\n\n\n\n## 3. Parallel Implementation\n\n略\n\n## 4. Experimental Results\n\n​\t对比实验使用包含各种英语文本和书籍的共计1181041个单词的布朗语料库，前800000单词用作训练，接下来200000用作验证集（模型选择、权重衰减、early stopping），剩余181041用作测试。不同单词的数目有47578（包括标点符号、大小写以及文本段落标识符），将频率<=3的稀有词合并为单个词，减小词表大小为$|V|=16383$\n\n​\t第二个实验使用1995到1996年的Associated Press(AP) News。训练集大约为1400万（13994528）单词，验证集约为100万（963138）单词，测试集也约为100万（963071）单词。原始数据共计148721单词（包括标点符号），减少为$|V|=17964$，处理方法是保留常用词（保留标点符号），大写转为小写，数字映射为特殊符号，稀有词及专有词映射为特殊符号\n\n​\t为了训练神经网络，初始学习率设为$\\epsilon_o = 10^{-3}$（在使用多个小数据尝试后），并根据如下规则衰减$\\epsilon_t = \\frac{\\epsilon_o}{1 + rt}$，t表示完成的参数更新次数，r表示参数衰减因子 $r = 10^{-8}$\n\n### 4.1 N-Gram Models\n\n第一个对比模型是采用插值法（interpolated）或平滑法（smoothed）的trigram 模型（Jelinek and Mercer, 1980）。模型的条件概率为\n$$\n\\hat{p}(w_t|w_{t-1}, w_{t-2}) = \\alpha_0(q_t)p_0 + \\alpha_1(q_t)p_1(w_t) + \\alpha_2(q_t)p_2(w_t|w_{t-1}) + \\alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})\n$$\n其中条件权重$\\alpha_i(q_t) \\geq 0, \\sum_i \\alpha_i(q_t) = 1$，$p_0 = 1/|V|$， $p_1(i)$为unigram（单词$i$在训练集中的相对频率），$p_2(i|j)$为bigram（$\\frac{词组(j,i)}{词(j)}$），$p_3(i|j,k)$ 为trigram（$\\frac{词组(k,j,i)}{词(j,i)}$）.对每个离散值$q_t$都有混合权重$\\alpha$，可通过EM算法经过大约5次迭代估算出。\n\n### 4.2 Results\n\n下图为不同模型在困惑度上的表现，可以看到神经网络模型表现最好\n\n![](/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg)\n\n## 5. Conclusion\n\n​\t实验作用在两个语料库中，一个超过100万数据，另一个超过1500万单词，都证明了本方法比state-of-art方法比如smoothed trigram优秀，困惑度低于10%到20%\n\n​\t我们坚信，主要原因在于利用了可学习的分布式表示来解决维度灾难，每个训练语句都存在相当的组合数目的其他邻居语句来训练模型\n\n​\t当然，模型还存在较多可改进之处，比如架构、计算性能、先验知识等，未来重点可放在改进加速技术，以及不增加训练时间的基础上增加容量（处理数亿个单词或更多）。利用时间结构扩展输入窗口以包含整个段落（无需增加参数数目或计算时间）的简单想法是使用time-delay或者rnn。\n\n## 6. 参考\n\n1. [A Neural Probabilistic Language Model](https://www.cnblogs.com/Dream-Fish/p/3950024.html)\n\n1. [NNLM 的 PyTorch 实现](https://wmathor.com/index.php/archives/1442/)\n\n## 7. 代码实现\n\n[DreamExplorerX-nnlm](https://github.com/DreamExplorerX/NLP-Garden/tree/master/1-1.NNLM)\n\n","slug":"A-Neural-Probabilistic-Language-Model","published":1,"updated":"2020-08-25T03:48:32.282Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke9etpdv0001zdr93d04ggkg","content":"<h1 id=\"A-Neural-Probabilistic-Language-Model\"><a href=\"#A-Neural-Probabilistic-Language-Model\" class=\"headerlink\" title=\"A Neural Probabilistic Language Model\"></a>A Neural Probabilistic Language Model</h1><p>​    本<a href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" target=\"_blank\" rel=\"noopener\">paper</a>由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>​    统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，导致目标非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列（这样导致高纬度下的数据系数会导致很多单词序列的概率为0）。<span style=\"color:red;\">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）根据其分布式表示得到的单词序列的概率函数。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>​    语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。</p>\n<p>​    统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\\hat{P}(w^T_1) = \\prod^T_{t=1} \\hat{P}(w_t|w^{t-1}<em>1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w</em>{i+1}, …, w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。</p>\n<p>​    当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个上下文，<code>n-gram</code>模型构建了上下文和下一个单词的条件概率表，例如前$n-1$个单词的组合：$$\\hat{p}(w_t|w^{t-1}<em>1) \\approx \\hat{p}(w_t|w^{t-1}</em>{t-n+1})$$，我们仅考虑在训练预料中真实存在或出现频率足够的连续单词的组合。但是若某个包含<code>n</code>个单词的单词序列未在训练预料中出现过怎么办？对于这种情况，并不能将其概率赋为$0$，因为这样的单词序列是很有可能会在更大的上下文中出现的。一个简单的方法是使用更小的上下文大小来预测其概率，正如论文back-off trigram model(Katz, 1987)以及smoothed(or interpolated) trigram models(Jelinek and Mercer, 1980)所示。因此，在这样的模型中，如何从训练预料中见过的单词序列中获得对新的单词序列的泛化能力呢？一种对此类模型获得泛化能力的理解是将此类interpolated或back-off n-gram模型思考为生成模型。最终，新的单词序列可通过“粘合(gluing)”训练语料中常见的较短、重叠的长度为1,2…到n个单词的序列块生成。在这种back-off或interpolated n-gram模型中，得到下一个单词的规则是隐式的。研究者在使用$n=3$，即trigrams中得到了state-of-art 结果，并且关于结合技巧来得到实质性改进的详细请看Goodman(2001). 显然，距离预测单词越近的前n个单词所包含的信息更多，但上述方法至少有两个方面值得改进，且本文也着重关注：1. 未考虑超过1或者2的更大的上下文；2. 未考虑单词间的相似度。比如，若训练语料中存在句子”The cat is walking in the bedroom”，则类似句子”A dog was running in a room”应该具有更高可能性，因为”dog”和”cat”(“the”和”a”, “room” 和”bedroom” 等等)在语义和语法上很相似。</p>\n<p><strong>Contributions</strong></p>\n<ol>\n<li>提出单词的分布式表示，解决维度灾难问题</li>\n<li>解决单词的长距离依赖限制，而ngram模型中一般$n=3$</li>\n<li>词的相似关系，本论文中单词以分布式表示，能够表示单词间的相似性</li>\n</ol>\n<h3 id=\"1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations\"><a href=\"#1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations\" class=\"headerlink\" title=\"1.1 Fighting the Curse of Dimensionality with Distributed Representations\"></a>1.1 Fighting the Curse of Dimensionality with Distributed Representations</h3><p>​    简而言之，本文提出的方法可归纳为如下几点</p>\n<ol>\n<li>将词典中的每个词都关联到一个分布式的<em>word feature vector</em>（一个$\\mathbb{R}^m$空间的实值向量）</li>\n<li>单词序列中的联合<em>probability function</em>通过单词的特征向量表征</li>\n<li>同时学习<em>word feature vectors</em>和<em>probability function</em>的参数</li>\n</ol>\n<p>​    单词的特征向量表征单词的不同方面：每个单词都关联到向量空间中的某个点。特征数目（比如m=30,60或者100）远小于词典大小（比如17000）。概率函数表示为在给定之前单词序列时，预测下一个单词的条件概率的乘积（比如实验中在给定先前单词时，采用了多层神经网络预测下一个单词）。概率函数的参数可被迭代调整以达到<strong>maxmize the log-likelihood of training data最大化训练数据的似然函数</strong>或者正则标准，比如增加一个权重衰减惩罚项。最后，每个单词的特征向量都可被学习，当然可采用语义特征的先验知识初始化。</p>\n<p>​    该方法为什么有效呢？在先前例子所示，若<em>dog<em>和</em>cat*的语法和语义相似，并且</em>(the, a), (bedroom, room), (is, was), (running, walking)<em>也类似，则很自然的可从*</em>The cat is walking in the bedroom** 生成<strong>A dog was running in a room</strong>，并且类似的<strong>The cat is running in a room</strong>、<strong>A dog is walking in a bedroom</strong>等等等其他各种组合。本文提出的模型是可以泛化的，因为<strong>相似的</strong>单词期望拥有相似的特征向量，并且概率函数是特征向量的<em>smooth平滑</em>函数，即特征上的微小变化也只会引起概率的微小变化。因此，训练数据中的上述句子的出现会增加概率，当然不仅仅是上述句子，还包括其在语句空间中组合数目的<em>邻居</em>句子（通过特征向量表达的句子）</p>\n<h3 id=\"1-2-Relation-to-Previous-Work\"><a href=\"#1-2-Relation-to-Previous-Work\" class=\"headerlink\" title=\"1.2 Relation to Previous Work\"></a>1.2 Relation to Previous Work</h3><p>略</p>\n<h2 id=\"2-A-Neural-Model\"><a href=\"#2-A-Neural-Model\" class=\"headerlink\" title=\"2. A Neural Model\"></a>2. A Neural Model</h2><p>​    训练集是诸如$w_1 … w_T$的单词序列，其中单词$w_t \\in V$，而词典$V$是一个巨大但有限的集合。目标是学习一个好模型<br>$$<br>f(w_t, …, w_{t-n+1}) = \\hat{p}(w_t|w^{t-1}<em>1)<br>$$<br>下面，我们给出概率的几何倒数$\\frac{1}{\\hat{p}(w_t|w^{t-1}_1)}$，也被称为<em>perplexity困惑度</em>，也是平均负似然函数的指数。模型的唯一约束在于对于任何的$w^{t-1}_1$， $\\sum^{|V|}</em>{i=1}f(i, w_{t-1}, …, w_{t-n+1}) = 1, f &gt; 0$，通过将这些条件概率相乘，得到单词序列的联合概率模型。</p>\n<p>​    我们将函数$f(w_t, …, w_{t-n+1}=\\hat{p}(w_t|w^{t-1}_1)$分解为两部分</p>\n<ol>\n<li><p>一个映射函数$C$， 它将词典$V$中的任意元素$i$映射为一个实值向量$C(i) \\in \\mathbb{R}^m$。它表示了词典中每个单词的<em>分布式特征向量 distributed feature vectors</em>。实际上，$C$是一个无参数的$|V| * m$的矩阵</p>\n</li>\n<li><p>一个由$C$表达的单词的概率函数：上下文单词的特征向量序列$(C(w_{t-n+1}), …, C(w_{t-1}))$作为函数$g$的输入，映射到词典$V$中下一个单词$w_t$的条件概率分布。函数$g$的输出是一个向量，其中第$i$-th 元素表示概率$\\hat{p}(w_t = i|w^{t-1}<em>1)$ ，如图1所示<br>$$<br>f(i, w</em>{t-1}, …, w_{t-n+1}) = g(i, C(w_{t-1}), …, C(w_{t-n+1}))<br>$$</p>\n</li>\n</ol>\n<p>函数$f$是上述两个映射($C$和$g$)的组合，并且$C$对于上下文中的所有单词都是共享参数的。这两部分都与一些参数有关，映射函数$C$的参数就是特征向量本身，表示为一个$|V|*m$的矩阵，其中第$i$行$C(i)$是单词$i$的特征向量。函数$g$可通过一个前向传播网络或RNN或其他参数化函数实现，参数为$\\omega$。整体的参数集合为$\\Theta = (C, \\omega)$</p>\n<p><img src=\"/img/A-Neural-Probabilistic-Language-Model/figure1.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<p>​    训练过程是寻找最大化训练语料的penalized log-likelihood的参数$\\Theta$,<br>$$<br>L = \\frac{1}{T}\\sum_t log f(w_t, …, w_{t-n+1}; \\Theta) + R(\\Theta)<br>$$<br>其中$R(\\Theta)$是正则化项，比如，在实验中，$R$是一个权重衰减惩罚项，只作用于神经网络的权重和矩阵$C$，不包括偏置项biases</p>\n<p>​    在上述模型中，自由参数(free parameters)的数目只与词汇表大小$V$成线性关系。它也只与阶数$n$成线性关系：若引入更多共享参数结构，则比例因子可降为sub-linear次线性，比如使用time-decay神经网络或rnn（或两种网络的组合形式）</p>\n<p>​    在如下大多数实验中，除了word features mapping外，神经网络只有一个隐藏层，并且可选性的，存在从word features到输出的直接连接。因此，实际上，神经网络有两个隐藏层：1. the shared word features layer C 即共享单词特征向量层$C$， 没有任何非线性能力（无实质作用）2. the ordinay hyperbolic tangent hidden layer即普通双曲正切隐藏层。更准确的讲，神经网络计算如下函数，使用<em>softmax</em>输出层，保证概率值为正，且和为1<br>$$<br>\\hat{p}(w_t|w_{t-1}, …, w_{t-n+1}) = \\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}<br>$$<br>其中$y_i$是每个输出单词$i$的未归一化的log-概率，包含参数$b, W, U, d, H$的计算公式如下<br>$$<br>y = b + Wx + Utanh(d + Hx)<br>$$<br>其中，双曲正切tanh是element-by-element方式作用，$W$可选为0，表示无直接连接。$x$为word features layer单词特征向量层的激活向量，是矩阵$C$上的输入单词的特征向量的拼接向量<br>$$<br>x = (C(w_{t-1}), C(w_{t-2}), …, C(w_{t-n+1}))<br>$$<br>设$h$为隐藏层单元数目，$m$为每个单词的特征向量的维度数目。若不存在word features到输出的直接连接，则矩阵$W$为0.模型的自由参数是输出层偏置项$b$（包含$|V|$个元素），隐藏层偏置项$d$（包含$h$个元素），隐藏层到输出层的权重$U$（一个$|V| \\times h$的矩阵），word features 到输出的权重$W$（一个$|V| \\times (n-1)m$的矩阵），隐藏层权重$H$（一个$h \\times (n-1)m$的矩阵），以及word features $C$（一个$|V| \\times m$的矩阵）<br>$$<br>\\Theta = (b, d, W, U, H, C)<br>$$<br>自由参数的数目是$|V|(1 + nm + h) + h(1 + (n-1)m)$。主导因子是$|V|(nm + h)$，注意在里乱上，若只对权重$W$和$H$而不包含$C$上存在权重衰减，则$W$和$H$会收敛到0，而$C$会继续增长，当然在实际中使用随机梯度上升不会存在这种现象。</p>\n<p>​    在神经网络上执行随机梯度上升算法，则执行第t个单词的迭代更新公式如下<br>$$<br>\\Theta \\leftarrow \\Theta + \\epsilon \\frac{\\partial \\hat{P}(w_t|w_{t-1}, …, w_{t-n+1})}{\\partial \\Theta}<br>$$<br>其中$\\epsilon$表示学习率，注意大量的参数无需更新：对于大量单词$j$的$C(j)$单词特征没有出现在输入窗口中。</p>\n<h2 id=\"3-Parallel-Implementation\"><a href=\"#3-Parallel-Implementation\" class=\"headerlink\" title=\"3. Parallel Implementation\"></a>3. Parallel Implementation</h2><p>略</p>\n<h2 id=\"4-Experimental-Results\"><a href=\"#4-Experimental-Results\" class=\"headerlink\" title=\"4. Experimental Results\"></a>4. Experimental Results</h2><p>​    对比实验使用包含各种英语文本和书籍的共计1181041个单词的布朗语料库，前800000单词用作训练，接下来200000用作验证集（模型选择、权重衰减、early stopping），剩余181041用作测试。不同单词的数目有47578（包括标点符号、大小写以及文本段落标识符），将频率&lt;=3的稀有词合并为单个词，减小词表大小为$|V|=16383$</p>\n<p>​    第二个实验使用1995到1996年的Associated Press(AP) News。训练集大约为1400万（13994528）单词，验证集约为100万（963138）单词，测试集也约为100万（963071）单词。原始数据共计148721单词（包括标点符号），减少为$|V|=17964$，处理方法是保留常用词（保留标点符号），大写转为小写，数字映射为特殊符号，稀有词及专有词映射为特殊符号</p>\n<p>​    为了训练神经网络，初始学习率设为$\\epsilon_o = 10^{-3}$（在使用多个小数据尝试后），并根据如下规则衰减$\\epsilon_t = \\frac{\\epsilon_o}{1 + rt}$，t表示完成的参数更新次数，r表示参数衰减因子 $r = 10^{-8}$</p>\n<h3 id=\"4-1-N-Gram-Models\"><a href=\"#4-1-N-Gram-Models\" class=\"headerlink\" title=\"4.1 N-Gram Models\"></a>4.1 N-Gram Models</h3><p>第一个对比模型是采用插值法（interpolated）或平滑法（smoothed）的trigram 模型（Jelinek and Mercer, 1980）。模型的条件概率为<br>$$<br>\\hat{p}(w_t|w_{t-1}, w_{t-2}) = \\alpha_0(q_t)p_0 + \\alpha_1(q_t)p_1(w_t) + \\alpha_2(q_t)p_2(w_t|w_{t-1}) + \\alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})<br>$$<br>其中条件权重$\\alpha_i(q_t) \\geq 0, \\sum_i \\alpha_i(q_t) = 1$，$p_0 = 1/|V|$， $p_1(i)$为unigram（单词$i$在训练集中的相对频率），$p_2(i|j)$为bigram（$\\frac{词组(j,i)}{词(j)}$），$p_3(i|j,k)$ 为trigram（$\\frac{词组(k,j,i)}{词(j,i)}$）.对每个离散值$q_t$都有混合权重$\\alpha$，可通过EM算法经过大约5次迭代估算出。</p>\n<h3 id=\"4-2-Results\"><a href=\"#4-2-Results\" class=\"headerlink\" title=\"4.2 Results\"></a>4.2 Results</h3><p>下图为不同模型在困惑度上的表现，可以看到神经网络模型表现最好</p>\n<p><img src=\"/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h2><p>​    实验作用在两个语料库中，一个超过100万数据，另一个超过1500万单词，都证明了本方法比state-of-art方法比如smoothed trigram优秀，困惑度低于10%到20%</p>\n<p>​    我们坚信，主要原因在于利用了可学习的分布式表示来解决维度灾难，每个训练语句都存在相当的组合数目的其他邻居语句来训练模型</p>\n<p>​    当然，模型还存在较多可改进之处，比如架构、计算性能、先验知识等，未来重点可放在改进加速技术，以及不增加训练时间的基础上增加容量（处理数亿个单词或更多）。利用时间结构扩展输入窗口以包含整个段落（无需增加参数数目或计算时间）的简单想法是使用time-delay或者rnn。</p>\n<h2 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6. 参考\"></a>6. 参考</h2><ol>\n<li><p><a href=\"https://www.cnblogs.com/Dream-Fish/p/3950024.html\" target=\"_blank\" rel=\"noopener\">A Neural Probabilistic Language Model</a></p>\n</li>\n<li><p><a href=\"https://wmathor.com/index.php/archives/1442/\" target=\"_blank\" rel=\"noopener\">NNLM 的 PyTorch 实现</a></p>\n</li>\n</ol>\n<h2 id=\"7-代码实现\"><a href=\"#7-代码实现\" class=\"headerlink\" title=\"7. 代码实现\"></a>7. 代码实现</h2><p><a href=\"https://github.com/DreamExplorerX/NLP-Garden/tree/master/1-1.NNLM\" target=\"_blank\" rel=\"noopener\">DreamExplorerX-nnlm</a></p>\n","site":{"data":{"_config":{"favicon":"/img/favicon.png","apple_touch_icon":"/img/favicon.png","title_join_string":" - ","force_https":false,"highlight":{"enable":true,"style":"Github Gist","bg_color":false,"copy_btn":true},"fun_features":{"typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"mouse_click":{"enable":false,"style":"values"}},"color":{"body_bg_color":"#eee","navbar_bg_color":"#2f4154","navbar_text_color":"white","text_color":"#3c4858","sec_text_color":"#718096","post_text_color":"#2c3e50","post_heading_color":"#1a202c","link_color":"#3c4858","link_hover_color":"#1abc9c","link_hover_bg_color":"#f8f9fa","board_color":"#fff"},"font":{"font_size":"16px","font_family":null,"code_font_size":"85%"},"custom_js":null,"custom_css":null,"custom_html":"","web_analytics":{"enable":false,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"tajs":null,"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"lazyload":{"enable":true,"onlypost":false},"daovoice":{"enable":false,"appid":""},"aplayer":{"enable":false,"autoplay":false,"loop":"all","order":"random","theme":"#b7daff","songs":[{"name":"name","artist":"artist","url":"/songs/test.mp3","cover":"/img/cover.jpg"},{"name":"name","artist":"artist","url":"https://...url.mp3","cover":"https://...cover.jpg"}]},"version":{"check":false},"navbar":{"blog_title":"Fluid","ground_glass":{"enable":false,"px":3,"alpha":0.7},"menu":[{"key":"home","link":"/","icon":"iconfont icon-home-fill"},{"key":"archive","link":"/archives/","icon":"iconfont icon-archive-fill"},{"key":"category","link":"/categories/","icon":"iconfont icon-category-fill"},{"key":"tag","link":"/tags/","icon":"iconfont icon-tags-fill"},{"key":"about","link":"/about/","icon":"iconfont icon-user-fill"}]},"search":{"enable":true,"path":"/local-search.xml","generate_path":"/local-search.xml","field":"post","content":true},"scroll_down_arrow":{"enable":true,"banner_height_limit":90,"scroll_after_turning_page":true},"banner_parallax":true,"footer":{"statistics":{"enable":false,"source":"busuanzi","pv_format":"总访问量 {} 次","uv_format":"总访客数 {} 人"},"beian":{"enable":false,"icp_text":"京ICP证123456号","police_text":"京公网安备12345678号","police_code":12345678,"police_icon":"/img/police_beian.png"}},"scroll_top_arrow":{"enable":true},"index":{"banner_img":"/img/default.png","banner_img_height":100,"banner_mask_alpha":0.3,"post_default_img":"","slogan":{"enable":true,"text":"An elegant Material-Design theme for Hexo"},"auto_excerpt":{"enable":true},"post_url_target":"_self","post_meta":{"date":true,"category":true,"tag":true}},"page":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3},"post":{"banner_img":"/img/default.png","banner_img_height":70,"banner_mask_alpha":0.3,"meta":{"date":{"enable":true,"format":"LL a"},"wordcount":{"enable":true,"format":"{} 字"},"min2read":{"enable":true,"format":"{} 分钟","words":100},"views":{"enable":false,"source":"busuanzi","format":"{} 次"}},"updated":{"enable":false,"content":"本文最后更新于：","relative":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"copyright":{"enable":true,"content":"本博客所有文章除特别声明外，均采用 <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.zh\" rel=\"nofollow noopener\">CC BY-SA 4.0 协议</a> ，转载请注明出处！"},"prev_next":{"enable":true},"custom":{"enable":false,"content":"<img src=\"https://octodex.github.com/images/jetpacktocat.png\" class=\"rounded mx-auto d-block mt-5\" style=\"width:150px; height:150px;\">"},"image_zoom":{"enable":true},"footnote":{"enable":true,"header":""},"math":{"enable":false,"specific":false,"engine":"mathjax"},"mermaid":{"enable":false,"specific":false,"options":{"theme":"default"}},"comments":{"enable":false,"type":"disqus"}},"utterances":{"repo":null,"issue_term":null,"label":"utterances","theme":"github-light","crossorigin":"anonymous"},"disqus":{"shortname":null,"disqusjs":false,"apikey":null},"gitalk":{"clientID":null,"clientSecret":null,"repo":null,"owner":null,"admin":null,"id":"location.pathname","language":"zh-CN","labels":"['Gitalk']","perPage":15,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true},"valine":{"appid":null,"appkey":null,"placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":null},"changyan":{"appid":"","appkey":""},"livere":{"uid":""},"archive":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null},"category":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"post_limit":10,"order_by":"-length"},"tag":{"banner_img":"/img/default.png","banner_img_height":80,"banner_mask_alpha":0.3,"subtitle":null,"tagcloud":{"min_font":15,"max_font":30,"unit":"px","start_color":"#BBBBEE","end_color":"#337ab7"}},"about":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"avatar":"/img/avatar.png","name":"myname","introduce":"一句简短的介绍","icons":[{"class":"iconfont icon-github-fill","link":"https://github.com","tip":"GitHub"},{"class":"iconfont icon-douban-fill","link":"https://douban.com","tip":"豆瓣"},{"class":"iconfont icon-wechat-fill","qrcode":"/img/favicon.png"}]},"page404":{"banner_img":"/img/default.png","banner_img_height":85,"banner_mask_alpha":0.3,"subtitle":"Page not found"},"links":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"items":[{"title":"Fluid Docs","intro":"主题使用指南","link":"https://hexo.fluid-dev.com/docs/","image":"/img/favicon.png"},{"title":"Fluid Repo","intro":"主题 GitHub 仓库","link":"https://github.com/fluid-dev/hexo-theme-fluid","image":"/img/favicon.png"},{"title":"Fluid Example","intro":"主题操作示例","link":"https://hexo.fluid-dev.com/docs/example/","image":"/img/favicon.png"}]}}}},"excerpt":"","more":"<h1 id=\"A-Neural-Probabilistic-Language-Model\"><a href=\"#A-Neural-Probabilistic-Language-Model\" class=\"headerlink\" title=\"A Neural Probabilistic Language Model\"></a>A Neural Probabilistic Language Model</h1><p>​    本<a href=\"http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\" target=\"_blank\" rel=\"noopener\">paper</a>由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。</p>\n<h2 id=\"Abstract\"><a href=\"#Abstract\" class=\"headerlink\" title=\"Abstract\"></a>Abstract</h2><p>​    统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，导致目标非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列（这样导致高纬度下的数据系数会导致很多单词序列的概率为0）。<span style=\"color:red;\">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）根据其分布式表示得到的单词序列的概率函数。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。</p>\n<h2 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h2><p>​    语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。</p>\n<p>​    统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\\hat{P}(w^T_1) = \\prod^T_{t=1} \\hat{P}(w_t|w^{t-1}<em>1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w</em>{i+1}, …, w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。</p>\n<p>​    当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个上下文，<code>n-gram</code>模型构建了上下文和下一个单词的条件概率表，例如前$n-1$个单词的组合：$$\\hat{p}(w_t|w^{t-1}<em>1) \\approx \\hat{p}(w_t|w^{t-1}</em>{t-n+1})$$，我们仅考虑在训练预料中真实存在或出现频率足够的连续单词的组合。但是若某个包含<code>n</code>个单词的单词序列未在训练预料中出现过怎么办？对于这种情况，并不能将其概率赋为$0$，因为这样的单词序列是很有可能会在更大的上下文中出现的。一个简单的方法是使用更小的上下文大小来预测其概率，正如论文back-off trigram model(Katz, 1987)以及smoothed(or interpolated) trigram models(Jelinek and Mercer, 1980)所示。因此，在这样的模型中，如何从训练预料中见过的单词序列中获得对新的单词序列的泛化能力呢？一种对此类模型获得泛化能力的理解是将此类interpolated或back-off n-gram模型思考为生成模型。最终，新的单词序列可通过“粘合(gluing)”训练语料中常见的较短、重叠的长度为1,2…到n个单词的序列块生成。在这种back-off或interpolated n-gram模型中，得到下一个单词的规则是隐式的。研究者在使用$n=3$，即trigrams中得到了state-of-art 结果，并且关于结合技巧来得到实质性改进的详细请看Goodman(2001). 显然，距离预测单词越近的前n个单词所包含的信息更多，但上述方法至少有两个方面值得改进，且本文也着重关注：1. 未考虑超过1或者2的更大的上下文；2. 未考虑单词间的相似度。比如，若训练语料中存在句子”The cat is walking in the bedroom”，则类似句子”A dog was running in a room”应该具有更高可能性，因为”dog”和”cat”(“the”和”a”, “room” 和”bedroom” 等等)在语义和语法上很相似。</p>\n<p><strong>Contributions</strong></p>\n<ol>\n<li>提出单词的分布式表示，解决维度灾难问题</li>\n<li>解决单词的长距离依赖限制，而ngram模型中一般$n=3$</li>\n<li>词的相似关系，本论文中单词以分布式表示，能够表示单词间的相似性</li>\n</ol>\n<h3 id=\"1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations\"><a href=\"#1-1-Fighting-the-Curse-of-Dimensionality-with-Distributed-Representations\" class=\"headerlink\" title=\"1.1 Fighting the Curse of Dimensionality with Distributed Representations\"></a>1.1 Fighting the Curse of Dimensionality with Distributed Representations</h3><p>​    简而言之，本文提出的方法可归纳为如下几点</p>\n<ol>\n<li>将词典中的每个词都关联到一个分布式的<em>word feature vector</em>（一个$\\mathbb{R}^m$空间的实值向量）</li>\n<li>单词序列中的联合<em>probability function</em>通过单词的特征向量表征</li>\n<li>同时学习<em>word feature vectors</em>和<em>probability function</em>的参数</li>\n</ol>\n<p>​    单词的特征向量表征单词的不同方面：每个单词都关联到向量空间中的某个点。特征数目（比如m=30,60或者100）远小于词典大小（比如17000）。概率函数表示为在给定之前单词序列时，预测下一个单词的条件概率的乘积（比如实验中在给定先前单词时，采用了多层神经网络预测下一个单词）。概率函数的参数可被迭代调整以达到<strong>maxmize the log-likelihood of training data最大化训练数据的似然函数</strong>或者正则标准，比如增加一个权重衰减惩罚项。最后，每个单词的特征向量都可被学习，当然可采用语义特征的先验知识初始化。</p>\n<p>​    该方法为什么有效呢？在先前例子所示，若<em>dog<em>和</em>cat*的语法和语义相似，并且</em>(the, a), (bedroom, room), (is, was), (running, walking)<em>也类似，则很自然的可从*</em>The cat is walking in the bedroom** 生成<strong>A dog was running in a room</strong>，并且类似的<strong>The cat is running in a room</strong>、<strong>A dog is walking in a bedroom</strong>等等等其他各种组合。本文提出的模型是可以泛化的，因为<strong>相似的</strong>单词期望拥有相似的特征向量，并且概率函数是特征向量的<em>smooth平滑</em>函数，即特征上的微小变化也只会引起概率的微小变化。因此，训练数据中的上述句子的出现会增加概率，当然不仅仅是上述句子，还包括其在语句空间中组合数目的<em>邻居</em>句子（通过特征向量表达的句子）</p>\n<h3 id=\"1-2-Relation-to-Previous-Work\"><a href=\"#1-2-Relation-to-Previous-Work\" class=\"headerlink\" title=\"1.2 Relation to Previous Work\"></a>1.2 Relation to Previous Work</h3><p>略</p>\n<h2 id=\"2-A-Neural-Model\"><a href=\"#2-A-Neural-Model\" class=\"headerlink\" title=\"2. A Neural Model\"></a>2. A Neural Model</h2><p>​    训练集是诸如$w_1 … w_T$的单词序列，其中单词$w_t \\in V$，而词典$V$是一个巨大但有限的集合。目标是学习一个好模型<br>$$<br>f(w_t, …, w_{t-n+1}) = \\hat{p}(w_t|w^{t-1}<em>1)<br>$$<br>下面，我们给出概率的几何倒数$\\frac{1}{\\hat{p}(w_t|w^{t-1}_1)}$，也被称为<em>perplexity困惑度</em>，也是平均负似然函数的指数。模型的唯一约束在于对于任何的$w^{t-1}_1$， $\\sum^{|V|}</em>{i=1}f(i, w_{t-1}, …, w_{t-n+1}) = 1, f &gt; 0$，通过将这些条件概率相乘，得到单词序列的联合概率模型。</p>\n<p>​    我们将函数$f(w_t, …, w_{t-n+1}=\\hat{p}(w_t|w^{t-1}_1)$分解为两部分</p>\n<ol>\n<li><p>一个映射函数$C$， 它将词典$V$中的任意元素$i$映射为一个实值向量$C(i) \\in \\mathbb{R}^m$。它表示了词典中每个单词的<em>分布式特征向量 distributed feature vectors</em>。实际上，$C$是一个无参数的$|V| * m$的矩阵</p>\n</li>\n<li><p>一个由$C$表达的单词的概率函数：上下文单词的特征向量序列$(C(w_{t-n+1}), …, C(w_{t-1}))$作为函数$g$的输入，映射到词典$V$中下一个单词$w_t$的条件概率分布。函数$g$的输出是一个向量，其中第$i$-th 元素表示概率$\\hat{p}(w_t = i|w^{t-1}<em>1)$ ，如图1所示<br>$$<br>f(i, w</em>{t-1}, …, w_{t-n+1}) = g(i, C(w_{t-1}), …, C(w_{t-n+1}))<br>$$</p>\n</li>\n</ol>\n<p>函数$f$是上述两个映射($C$和$g$)的组合，并且$C$对于上下文中的所有单词都是共享参数的。这两部分都与一些参数有关，映射函数$C$的参数就是特征向量本身，表示为一个$|V|*m$的矩阵，其中第$i$行$C(i)$是单词$i$的特征向量。函数$g$可通过一个前向传播网络或RNN或其他参数化函数实现，参数为$\\omega$。整体的参数集合为$\\Theta = (C, \\omega)$</p>\n<p><img src=\"/img/A-Neural-Probabilistic-Language-Model/figure1.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<p>​    训练过程是寻找最大化训练语料的penalized log-likelihood的参数$\\Theta$,<br>$$<br>L = \\frac{1}{T}\\sum_t log f(w_t, …, w_{t-n+1}; \\Theta) + R(\\Theta)<br>$$<br>其中$R(\\Theta)$是正则化项，比如，在实验中，$R$是一个权重衰减惩罚项，只作用于神经网络的权重和矩阵$C$，不包括偏置项biases</p>\n<p>​    在上述模型中，自由参数(free parameters)的数目只与词汇表大小$V$成线性关系。它也只与阶数$n$成线性关系：若引入更多共享参数结构，则比例因子可降为sub-linear次线性，比如使用time-decay神经网络或rnn（或两种网络的组合形式）</p>\n<p>​    在如下大多数实验中，除了word features mapping外，神经网络只有一个隐藏层，并且可选性的，存在从word features到输出的直接连接。因此，实际上，神经网络有两个隐藏层：1. the shared word features layer C 即共享单词特征向量层$C$， 没有任何非线性能力（无实质作用）2. the ordinay hyperbolic tangent hidden layer即普通双曲正切隐藏层。更准确的讲，神经网络计算如下函数，使用<em>softmax</em>输出层，保证概率值为正，且和为1<br>$$<br>\\hat{p}(w_t|w_{t-1}, …, w_{t-n+1}) = \\frac{e^{y_{w_t}}}{\\sum_i e^{y_i}}<br>$$<br>其中$y_i$是每个输出单词$i$的未归一化的log-概率，包含参数$b, W, U, d, H$的计算公式如下<br>$$<br>y = b + Wx + Utanh(d + Hx)<br>$$<br>其中，双曲正切tanh是element-by-element方式作用，$W$可选为0，表示无直接连接。$x$为word features layer单词特征向量层的激活向量，是矩阵$C$上的输入单词的特征向量的拼接向量<br>$$<br>x = (C(w_{t-1}), C(w_{t-2}), …, C(w_{t-n+1}))<br>$$<br>设$h$为隐藏层单元数目，$m$为每个单词的特征向量的维度数目。若不存在word features到输出的直接连接，则矩阵$W$为0.模型的自由参数是输出层偏置项$b$（包含$|V|$个元素），隐藏层偏置项$d$（包含$h$个元素），隐藏层到输出层的权重$U$（一个$|V| \\times h$的矩阵），word features 到输出的权重$W$（一个$|V| \\times (n-1)m$的矩阵），隐藏层权重$H$（一个$h \\times (n-1)m$的矩阵），以及word features $C$（一个$|V| \\times m$的矩阵）<br>$$<br>\\Theta = (b, d, W, U, H, C)<br>$$<br>自由参数的数目是$|V|(1 + nm + h) + h(1 + (n-1)m)$。主导因子是$|V|(nm + h)$，注意在里乱上，若只对权重$W$和$H$而不包含$C$上存在权重衰减，则$W$和$H$会收敛到0，而$C$会继续增长，当然在实际中使用随机梯度上升不会存在这种现象。</p>\n<p>​    在神经网络上执行随机梯度上升算法，则执行第t个单词的迭代更新公式如下<br>$$<br>\\Theta \\leftarrow \\Theta + \\epsilon \\frac{\\partial \\hat{P}(w_t|w_{t-1}, …, w_{t-n+1})}{\\partial \\Theta}<br>$$<br>其中$\\epsilon$表示学习率，注意大量的参数无需更新：对于大量单词$j$的$C(j)$单词特征没有出现在输入窗口中。</p>\n<h2 id=\"3-Parallel-Implementation\"><a href=\"#3-Parallel-Implementation\" class=\"headerlink\" title=\"3. Parallel Implementation\"></a>3. Parallel Implementation</h2><p>略</p>\n<h2 id=\"4-Experimental-Results\"><a href=\"#4-Experimental-Results\" class=\"headerlink\" title=\"4. Experimental Results\"></a>4. Experimental Results</h2><p>​    对比实验使用包含各种英语文本和书籍的共计1181041个单词的布朗语料库，前800000单词用作训练，接下来200000用作验证集（模型选择、权重衰减、early stopping），剩余181041用作测试。不同单词的数目有47578（包括标点符号、大小写以及文本段落标识符），将频率&lt;=3的稀有词合并为单个词，减小词表大小为$|V|=16383$</p>\n<p>​    第二个实验使用1995到1996年的Associated Press(AP) News。训练集大约为1400万（13994528）单词，验证集约为100万（963138）单词，测试集也约为100万（963071）单词。原始数据共计148721单词（包括标点符号），减少为$|V|=17964$，处理方法是保留常用词（保留标点符号），大写转为小写，数字映射为特殊符号，稀有词及专有词映射为特殊符号</p>\n<p>​    为了训练神经网络，初始学习率设为$\\epsilon_o = 10^{-3}$（在使用多个小数据尝试后），并根据如下规则衰减$\\epsilon_t = \\frac{\\epsilon_o}{1 + rt}$，t表示完成的参数更新次数，r表示参数衰减因子 $r = 10^{-8}$</p>\n<h3 id=\"4-1-N-Gram-Models\"><a href=\"#4-1-N-Gram-Models\" class=\"headerlink\" title=\"4.1 N-Gram Models\"></a>4.1 N-Gram Models</h3><p>第一个对比模型是采用插值法（interpolated）或平滑法（smoothed）的trigram 模型（Jelinek and Mercer, 1980）。模型的条件概率为<br>$$<br>\\hat{p}(w_t|w_{t-1}, w_{t-2}) = \\alpha_0(q_t)p_0 + \\alpha_1(q_t)p_1(w_t) + \\alpha_2(q_t)p_2(w_t|w_{t-1}) + \\alpha_3(q_t)p_3(w_t|w_{t-1},w_{t-2})<br>$$<br>其中条件权重$\\alpha_i(q_t) \\geq 0, \\sum_i \\alpha_i(q_t) = 1$，$p_0 = 1/|V|$， $p_1(i)$为unigram（单词$i$在训练集中的相对频率），$p_2(i|j)$为bigram（$\\frac{词组(j,i)}{词(j)}$），$p_3(i|j,k)$ 为trigram（$\\frac{词组(k,j,i)}{词(j,i)}$）.对每个离散值$q_t$都有混合权重$\\alpha$，可通过EM算法经过大约5次迭代估算出。</p>\n<h3 id=\"4-2-Results\"><a href=\"#4-2-Results\" class=\"headerlink\" title=\"4.2 Results\"></a>4.2 Results</h3><p>下图为不同模型在困惑度上的表现，可以看到神经网络模型表现最好</p>\n<p><img src=\"/img/A-Neural-Probabilistic-Language-Model/table1-results.jpg\" srcset=\"/img/loading.gif\" alt=\"\"></p>\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5. Conclusion\"></a>5. Conclusion</h2><p>​    实验作用在两个语料库中，一个超过100万数据，另一个超过1500万单词，都证明了本方法比state-of-art方法比如smoothed trigram优秀，困惑度低于10%到20%</p>\n<p>​    我们坚信，主要原因在于利用了可学习的分布式表示来解决维度灾难，每个训练语句都存在相当的组合数目的其他邻居语句来训练模型</p>\n<p>​    当然，模型还存在较多可改进之处，比如架构、计算性能、先验知识等，未来重点可放在改进加速技术，以及不增加训练时间的基础上增加容量（处理数亿个单词或更多）。利用时间结构扩展输入窗口以包含整个段落（无需增加参数数目或计算时间）的简单想法是使用time-delay或者rnn。</p>\n<h2 id=\"6-参考\"><a href=\"#6-参考\" class=\"headerlink\" title=\"6. 参考\"></a>6. 参考</h2><ol>\n<li><p><a href=\"https://www.cnblogs.com/Dream-Fish/p/3950024.html\" target=\"_blank\" rel=\"noopener\">A Neural Probabilistic Language Model</a></p>\n</li>\n<li><p><a href=\"https://wmathor.com/index.php/archives/1442/\" target=\"_blank\" rel=\"noopener\">NNLM 的 PyTorch 实现</a></p>\n</li>\n</ol>\n<h2 id=\"7-代码实现\"><a href=\"#7-代码实现\" class=\"headerlink\" title=\"7. 代码实现\"></a>7. 代码实现</h2><p><a href=\"https://github.com/DreamExplorerX/NLP-Garden/tree/master/1-1.NNLM\" target=\"_blank\" rel=\"noopener\">DreamExplorerX-nnlm</a></p>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2020-07-14T09:49:58.739Z","updated":"2020-07-14T09:49:58.739Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cke9etpez000czdr94yua0z1j","content":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"hljs bash\">$ hexo new <span class=\"hljs-string\">\"My New Post\"</span></code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"hljs bash\">$ hexo server</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"hljs bash\">$ hexo generate</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"hljs bash\">$ hexo deploy</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n","site":{"data":{"_config":{"favicon":"/img/favicon.png","apple_touch_icon":"/img/favicon.png","title_join_string":" - ","force_https":false,"highlight":{"enable":true,"style":"Github Gist","bg_color":false,"copy_btn":true},"fun_features":{"typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"mouse_click":{"enable":false,"style":"values"}},"color":{"body_bg_color":"#eee","navbar_bg_color":"#2f4154","navbar_text_color":"white","text_color":"#3c4858","sec_text_color":"#718096","post_text_color":"#2c3e50","post_heading_color":"#1a202c","link_color":"#3c4858","link_hover_color":"#1abc9c","link_hover_bg_color":"#f8f9fa","board_color":"#fff"},"font":{"font_size":"16px","font_family":null,"code_font_size":"85%"},"custom_js":null,"custom_css":null,"custom_html":"","web_analytics":{"enable":false,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"tajs":null,"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}},"lazyload":{"enable":true,"onlypost":false},"daovoice":{"enable":false,"appid":""},"aplayer":{"enable":false,"autoplay":false,"loop":"all","order":"random","theme":"#b7daff","songs":[{"name":"name","artist":"artist","url":"/songs/test.mp3","cover":"/img/cover.jpg"},{"name":"name","artist":"artist","url":"https://...url.mp3","cover":"https://...cover.jpg"}]},"version":{"check":false},"navbar":{"blog_title":"Fluid","ground_glass":{"enable":false,"px":3,"alpha":0.7},"menu":[{"key":"home","link":"/","icon":"iconfont icon-home-fill"},{"key":"archive","link":"/archives/","icon":"iconfont icon-archive-fill"},{"key":"category","link":"/categories/","icon":"iconfont icon-category-fill"},{"key":"tag","link":"/tags/","icon":"iconfont icon-tags-fill"},{"key":"about","link":"/about/","icon":"iconfont icon-user-fill"}]},"search":{"enable":true,"path":"/local-search.xml","generate_path":"/local-search.xml","field":"post","content":true},"scroll_down_arrow":{"enable":true,"banner_height_limit":90,"scroll_after_turning_page":true},"banner_parallax":true,"footer":{"statistics":{"enable":false,"source":"busuanzi","pv_format":"总访问量 {} 次","uv_format":"总访客数 {} 人"},"beian":{"enable":false,"icp_text":"京ICP证123456号","police_text":"京公网安备12345678号","police_code":12345678,"police_icon":"/img/police_beian.png"}},"scroll_top_arrow":{"enable":true},"index":{"banner_img":"/img/default.png","banner_img_height":100,"banner_mask_alpha":0.3,"post_default_img":"","slogan":{"enable":true,"text":"An elegant Material-Design theme for Hexo"},"auto_excerpt":{"enable":true},"post_url_target":"_self","post_meta":{"date":true,"category":true,"tag":true}},"page":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3},"post":{"banner_img":"/img/default.png","banner_img_height":70,"banner_mask_alpha":0.3,"meta":{"date":{"enable":true,"format":"LL a"},"wordcount":{"enable":true,"format":"{} 字"},"min2read":{"enable":true,"format":"{} 分钟","words":100},"views":{"enable":false,"source":"busuanzi","format":"{} 次"}},"updated":{"enable":false,"content":"本文最后更新于：","relative":false},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"copyright":{"enable":true,"content":"本博客所有文章除特别声明外，均采用 <a href=\"https://creativecommons.org/licenses/by-sa/4.0/deed.zh\" rel=\"nofollow noopener\">CC BY-SA 4.0 协议</a> ，转载请注明出处！"},"prev_next":{"enable":true},"custom":{"enable":false,"content":"<img src=\"https://octodex.github.com/images/jetpacktocat.png\" class=\"rounded mx-auto d-block mt-5\" style=\"width:150px; height:150px;\">"},"image_zoom":{"enable":true},"footnote":{"enable":true,"header":""},"math":{"enable":false,"specific":false,"engine":"mathjax"},"mermaid":{"enable":false,"specific":false,"options":{"theme":"default"}},"comments":{"enable":false,"type":"disqus"}},"utterances":{"repo":null,"issue_term":null,"label":"utterances","theme":"github-light","crossorigin":"anonymous"},"disqus":{"shortname":null,"disqusjs":false,"apikey":null},"gitalk":{"clientID":null,"clientSecret":null,"repo":null,"owner":null,"admin":null,"id":"location.pathname","language":"zh-CN","labels":"['Gitalk']","perPage":15,"pagerDirection":"last","distractionFreeMode":false,"createIssueManually":true},"valine":{"appid":null,"appkey":null,"placeholder":"说点什么","path":"window.location.pathname","avatar":"retro","meta":["nick","mail","link"],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":null},"changyan":{"appid":"","appkey":""},"livere":{"uid":""},"archive":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null},"category":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"post_limit":10,"order_by":"-length"},"tag":{"banner_img":"/img/default.png","banner_img_height":80,"banner_mask_alpha":0.3,"subtitle":null,"tagcloud":{"min_font":15,"max_font":30,"unit":"px","start_color":"#BBBBEE","end_color":"#337ab7"}},"about":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"avatar":"/img/avatar.png","name":"myname","introduce":"一句简短的介绍","icons":[{"class":"iconfont icon-github-fill","link":"https://github.com","tip":"GitHub"},{"class":"iconfont icon-douban-fill","link":"https://douban.com","tip":"豆瓣"},{"class":"iconfont icon-wechat-fill","qrcode":"/img/favicon.png"}]},"page404":{"banner_img":"/img/default.png","banner_img_height":85,"banner_mask_alpha":0.3,"subtitle":"Page not found"},"links":{"banner_img":"/img/default.png","banner_img_height":60,"banner_mask_alpha":0.3,"subtitle":null,"items":[{"title":"Fluid Docs","intro":"主题使用指南","link":"https://hexo.fluid-dev.com/docs/","image":"/img/favicon.png"},{"title":"Fluid Repo","intro":"主题 GitHub 仓库","link":"https://github.com/fluid-dev/hexo-theme-fluid","image":"/img/favicon.png"},{"title":"Fluid Example","intro":"主题操作示例","link":"https://hexo.fluid-dev.com/docs/example/","image":"/img/favicon.png"}]}}}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\" target=\"_blank\" rel=\"noopener\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\" target=\"_blank\" rel=\"noopener\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\" target=\"_blank\" rel=\"noopener\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\" target=\"_blank\" rel=\"noopener\">GitHub</a>.</p>\n<h2 id=\"Quick-Start\"><a href=\"#Quick-Start\" class=\"headerlink\" title=\"Quick Start\"></a>Quick Start</h2><h3 id=\"Create-a-new-post\"><a href=\"#Create-a-new-post\" class=\"headerlink\" title=\"Create a new post\"></a>Create a new post</h3><pre><code class=\"hljs bash\">$ hexo new <span class=\"hljs-string\">\"My New Post\"</span></code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\" target=\"_blank\" rel=\"noopener\">Writing</a></p>\n<h3 id=\"Run-server\"><a href=\"#Run-server\" class=\"headerlink\" title=\"Run server\"></a>Run server</h3><pre><code class=\"hljs bash\">$ hexo server</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/server.html\" target=\"_blank\" rel=\"noopener\">Server</a></p>\n<h3 id=\"Generate-static-files\"><a href=\"#Generate-static-files\" class=\"headerlink\" title=\"Generate static files\"></a>Generate static files</h3><pre><code class=\"hljs bash\">$ hexo generate</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\" target=\"_blank\" rel=\"noopener\">Generating</a></p>\n<h3 id=\"Deploy-to-remote-sites\"><a href=\"#Deploy-to-remote-sites\" class=\"headerlink\" title=\"Deploy to remote sites\"></a>Deploy to remote sites</h3><pre><code class=\"hljs bash\">$ hexo deploy</code></pre>\n\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\" target=\"_blank\" rel=\"noopener\">Deployment</a></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cke9etpdq0000zdr97mkae6pa","category_id":"cke9etpe10002zdr95kglf1rx","_id":"cke9etpe50005zdr91v0mgjgv"}],"PostTag":[{"post_id":"cke9etpdq0000zdr97mkae6pa","tag_id":"cke9etpe30003zdr9crvrge8k","_id":"cke9etpe60007zdr9fdktd1v6"},{"post_id":"cke9etpdq0000zdr97mkae6pa","tag_id":"cke9etpe40004zdr9d19h5rgd","_id":"cke9etpe60008zdr99oh6g6w8"},{"post_id":"cke9etpdv0001zdr93d04ggkg","tag_id":"cke9etpe50006zdr9cfqt4afr","_id":"cke9etpe8000azdr9au4l8jfm"},{"post_id":"cke9etpdv0001zdr93d04ggkg","tag_id":"cke9etpe60009zdr99aks7y6c","_id":"cke9etpe8000bzdr9cr702763"}],"Tag":[{"name":"C++11","_id":"cke9etpe30003zdr9crvrge8k"},{"name":"random","_id":"cke9etpe40004zdr9d19h5rgd"},{"name":"Deep Learning","_id":"cke9etpe50006zdr9cfqt4afr"},{"name":"NLP","_id":"cke9etpe60009zdr99aks7y6c"}]}}