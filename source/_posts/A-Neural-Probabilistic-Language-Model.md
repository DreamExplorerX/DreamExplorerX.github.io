---
title: A Neural Probabilistic Language Model
date: 2020-07-19 17:22:22
tags: [Deep Learning, NLP]
caetgories: [Deep Learning]
index_img: img/A-Neural-Probabilistic-Language-Model-Arch.png
banner_img: img/A-Neural-Probabilistic-Language-Model-Arch.png
---

# A Neural Probabilistic Language Model

本[paper](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)由深度学习奠基人Yoshua Bengio教授发表于2003，虽年代久远，却意义非凡。

## Abstract
统计语言模型的目标是学习语言中的单词序列的联合概率函数，但因维度灾难(curse of dimensionality)，从本质上讲非常困难。维度灾难是指:模型测试时的单词序列很可能不同于训练时的单词序列。<span style="color:red;">传统但比较成功的基于n-grams的方法通通过拼接训练集中较短的重叠的序列来获得泛化能力。</span> 我们提出了一种通过学习单词的分布式表示的方法解决维度灾难问题，针对每一个训练语句，该方法可以传递给模型其指数数量级的语义相关/近邻的句子。本模型可同时学习（1）每个单词都学习到一个分布式表示（2）单词序列的概率函数，根据它们的分布式表示表达。获得泛化能力的原因在于若某个单词序列从未见过，但组成该序列的单词却与构成训练集中单词序列的单词相似（在有近邻表示的意义上），则该序列会得到较高的概率。在合理时间内训练一个大型模型（数百万参数）是非常大的挑战。本报告使用神经网络作为概率函数，并在两个文本语料中证明本方法极大提升当前最优的n-gram模型，并且本方法可以利用更长的上下文。

## Introduction

语言模型或其他学习问题的一个根本性问题是维度灾难，尤其是对许多离散随机变量（比如语句中的单词、数据挖掘任务的离散属性）的联合概率分布建模时。比如，若某种自然语言词汇表大小$V$为100,000，若对包含10个连续单词的语句序列的联合分布建模，则参数有$100000^{10} - 1 = 10^{50} - 1$个。当建模连续变量时，可以比较容易获得泛化能力（比如，使用平滑类别函数例如多层神经网络MLP、高斯混合模型），因为期望的学习函数具有一些局部平滑特性。对于离散空间，泛化结构不明显，离散变量的任何变化都可能导致函数预测值的巨大影响，并且离散随机变量可取值很大，而大多数能观察到的对象相距很远（汉明距离）。

统计语言模型可表示为在给定前文时预测下一个单词的条件概率 $$\hat{P}(w^T_1) = \prod^T_{i=1} \hat{P}(w_t|w^{t-1}_1)$$ ，$w_t$表示第$t$个单词，子序列$w^j_i = (w_i, w_{i+1}, ..., w_{j-1}, w_j)$。这类统计语言模型在很多设计自然语言的技术领域被证明有用，包括语音识别，机器翻译和信息检索。

当构建自然语言统计模型时，可利用词序极大地降低建模问题的复杂度，并且单词序列中相近的单词有更强的依赖性。因此，对大量上下文的每个，`n-gram`模型构建了上下文和下一个单词的条件概率表：$$$$

